<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[关于博客那点事]]></title>
    <url>%2F2018%2F07%2F20%2FIntroduction%2F</url>
    <content type="text"><![CDATA[我从来没有感觉到与一个人如此心灵相通同时又如此无助直到那天我Baidu无果，想尽办法Google到一个问题那个人遇到过一个跟我一样的问题没有回帖发帖时间：1970年1月1日 请问此刻的你有什么获奖感言？是否很想问这位大侠后来呢？也许你会留下你的评论，“大侠我也遇到过同样的问题，请问后面是怎么解决的？大侠…大侠..你还在吗？”当然这只是一个笑话，关于博客，其实我一直有这样一个想法，想把自己的所遇所知分享给大家，也许在未来的某一天能有幸与你相遇。]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>随记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph-rest-api的IPv6环境配置]]></title>
    <url>%2F2017%2F12%2F26%2Fceph-rest-api%2F</url>
    <content type="text"><![CDATA[引言ceph-rest-api 是一个 WSGI （网页服务器网关接口）应用程序，可作为网页服务独立运行，也可在支持 WSGI 的网页服务器下运行。它通过 HTTP 访问接口提供了 ceph 命令行工具的大多数功能，官网详细介绍。本篇将介绍如何配置IPv6环境的ceph-rest-api。 配置IPv4环境ceph-rest-api默认已经安装在ceph集群中，使用如下命令即可启功：12[root@ceph001 ~]# ceph-rest-api -n client.admin * Running on http://0.0.0.0:5000/ 如果要更改IP或者端口号，只需在ceph.conf配置文件[global]下添加public_addr字段，如下：12[global]public_addr = 49.123.93.84:5001 启动ceph-rest-api12[root@ceph001 cluster]# ceph-rest-api -n client.admin * Running on http://49.123.93.84:5001/ 访问 http://49.123.93.84:5001/ 即可查看对应的原生API IPv6环境之前尝试直接在ceph.conf配置文件中配置对应的IPv6地址与端口号，但是在启动时报错123456789101112131415161718192021222324[root@ceph001 cluster]# ceph-rest-api -n client.admin * Running on http://[[2001:250:4402:2001:20c:29ff:fe25:8888]]:5001/Traceback (most recent call last): File "/usr/bin/ceph-rest-api", line 72, in &lt;module&gt; app.run(host=app.ceph_addr, port=app.ceph_port) File "/usr/lib/python2.7/site-packages/flask/app.py", line 772, in run run_simple(host, port, self, **options) File "/usr/lib/python2.7/site-packages/werkzeug/serving.py", line 710, in run_simple inner() File "/usr/lib/python2.7/site-packages/werkzeug/serving.py", line 692, in inner passthrough_errors, ssl_context).serve_forever() File "/usr/lib/python2.7/site-packages/werkzeug/serving.py", line 486, in make_server passthrough_errors, ssl_context) File "/usr/lib/python2.7/site-packages/werkzeug/serving.py", line 410, in __init__ HTTPServer.__init__(self, (host, int(port)), handler) File "/usr/lib64/python2.7/SocketServer.py", line 419, in __init__ self.server_bind() File "/usr/lib64/python2.7/BaseHTTPServer.py", line 108, in server_bind SocketServer.TCPServer.server_bind(self) File "/usr/lib64/python2.7/SocketServer.py", line 430, in server_bind self.socket.bind(self.server_address) File "/usr/lib64/python2.7/socket.py", line 224, in meth return getattr(self._sock,name)(*args)socket.gaierror: [Errno -2] Name or service not known 从报错信息可知IPv6地址不合法http://[[2001:250:4402:2001:20c:29ff:fe25:8888]]:5001/ ，正确地址应该是http://[2001:250:4402:2001:20c:29ff:fe25:8888]:5001/ ，找到对应的/usr/bin/ceph-rest-api文件，打印对应的host为[2001:250:4402:2001:20c:29ff:fe25:8888]，目前找到的解决办法是直接将IPv6地址与端口号写入此文件12345678...if 'pdb.py' in files: app.run(host=app.ceph_addr, port=app.ceph_port, debug=True, use_reloader=False, use_debugger=False)else: app.ceph_addr='2001:250:4402:2001:20c:29ff:fe25:8888'; app.ceph_port=5001; app.run(host=app.ceph_addr, port=app.ceph_port) 并运行ceph-rest-api12[root@ceph001 cluster]# ceph-rest-api -n client.admin * Running on http://[2001:250:4402:2001:20c:29ff:fe25:8888]:5001/ 访问地址 http://[2001:250:4402:2001:20c:29ff:fe25:8888]:5001/ 即可。 后续本篇中提到的配置IPv6的方法仅仅是权宜之计，按理来说不应该修改ceph-rest-api文件，而是去查看对应的url地址如何拼接，哪位大神如果知道在哪修改欢迎留言。]]></content>
      <categories>
        <category>Ceph</category>
      </categories>
      <tags>
        <tag>IPv6</tag>
        <tag>Ceph</tag>
        <tag>rgw</tag>
        <tag>WSGI</tag>
        <tag>ceph-rest-api</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7最小化安装问题小结]]></title>
    <url>%2F2017%2F07%2F19%2Fcentos7-minimal-summary%2F</url>
    <content type="text"><![CDATA[引言在最小化安装centos 7后，本着一贯低调做事的原则，输入ifconfig炫耀几番，扎心了，老铁！竟然提示我没有这个命令？ 问题1 ifconfig: command not found12[root@localhost ~]# ifconfig-bash: ifconfig: command not found 难道看个ip地址都不允许？一定是我打开的方式不对123456789[root@localhost ~]# ip addr show1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eno16777736: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:14:62:84 brd ff:ff:ff:ff:ff:ff 发现竟然都没给我分配IP地址，上网就更别想了，于是乎 编辑了其网络配置文件,手动添加ip地址，DNS以及网关等信息，然后重启网络123456789101112131415161718192021[root@localhost ~]# cat /etc/sysconfig/network-scripts/ifcfg-eno16777736TYPE=EthernetBOOTPROTO=noneDEFROUTE=yesPEERDNS=yesPEERROUTES=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_PEERDNS=yesIPV6_PEERROUTES=yesIPV6_FAILURE_FATAL=noNAME=eno16777736UUID=0b9430ac-5ccc-4f13-9280-4306975007cbDEVICE=eno16777736ONBOOT=yesDNS1=10.0.0.2IPADDR=10.0.0.100GATEWAY=10.0.0.2NETMASK=255.255.255.0 再次查看IP信息，并ping了一下 www.baidu.com123456789101112131415161718[root@localhost ~]# ip addr show1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eno16777736: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:14:62:84 brd ff:ff:ff:ff:ff:ff inet 10.0.0.100/24 brd 10.0.0.2]55 scope global eno16777736 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe14:6284/64 scope link valid_lft forever preferred_lft forever[root@localhost ~]# ping www.baidu.comPING www.a.shifen.com (119.75.216.20) 56(84) bytes of data.64 bytes from 119.75.216.20: icmp_seq=1 ttl=128 time=27.3 ms64 bytes from 119.75.216.20: icmp_seq=2 ttl=128 time=30.6 ms 上网的问题是解决了，但是总觉得这个查看IP的命令没ifconfig漂亮，因此利用yum命令安装了ifconfig(在net-tools包中)1234567891011121314151617181920212223[root@localhost ~]# yum install net-toolsLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile * base: mirrors.njupt.edu.cn * extras: mirror.bit.edu.cn * updates: mirror.bit.edu.cnResolving Dependencies--&gt; Running transaction check---&gt; Package net-tools.x86_64 0:2.0-0.17.20131004git.el7 will be installed--&gt; Finished Dependency ResolutionDependencies Resolved=============================================================================================================================================================================================== Package Arch Version Repository Size===============================================================================================================================================================================================Installing: net-tools x86_64 2.0-0.17.20131004git.el7 base 304 kTransaction Summary===============================================================================================================================================================================================Install 1 Package...... 大功告成，是不是觉得贼帅123456789101112131415161718[root@localhost ~]# ifconfigeno16777736: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.0.0.100 netmask 255.255.255.0 broadcast 10.0.0.255 inet6 fe80::20c:29ff:fe14:6284 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:14:62:84 txqueuelen 1000 (Ethernet) RX packets 206 bytes 20795 (20.3 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 176 bytes 23632 (23.0 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 0 (Local Loopback) RX packets 4 bytes 332 (332.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 4 bytes 332 (332.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0]]></content>
      <categories>
        <category>Linux 基础</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>CentOS7</tag>
        <tag>最小化安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新闻数据爬虫分析]]></title>
    <url>%2F2017%2F07%2F13%2Fnews-spider%2F</url>
    <content type="text"><![CDATA[引言最近接触到一个新闻类网站的数据爬取项目，包括各大新闻网站，如网易、腾讯、新浪等新闻标题、时间、评论数、点赞量、转发量以及阅读量等数据获取。其中较麻烦模块主要是动态数据的获取，以及新浪微博数据的获取，在此分享一些经验给大家参考，如有不对望指正交流、共同进步。 爬虫工具WebMagic爬虫工具，提供详细使用配置说明。 新闻网站格式分析及数据获取网易新闻新闻网址首页：http://news.163.com/新闻详情页地址格式：http://[\S].163.com/[\S].html，比如 http://news.163.com/17/0713/20/CP8JPMSD000189FH.html新闻时间格式：yyyy-MM-dd hh:mm:ss，如 2017-07-13 20:40:07，通过正则表达式匹配出新闻时间跟帖数：跟帖数为动态获取数据，不能直接从页面源文件中获取 评论地址：http://comment.news.163.com/api/v1/products/+productKey+/threads/+docId ,如 http://comment.news.163.com/api/v1/products/a2869674571f77b5a0867c3d71db5856/threads/CP8JPMSD000189FH ,返回Json数据格式如下12345678910111213141516171819202122232425&#123;"against": 0,"boardId": "news2_bbs","channelId": "0001","cmtAgainst": 0,"cmtVote": 0,"createTime": "2017-07-13 20:40:07","docId": "CP8JPMSD000189FH","isAudit": true,"modifyTime": "2017-07-13 20:58:24","pdocId": "CP8JPMSD000189FH","rcount": 63,"status": &#123;"app": "on","web": "on","audio": "off","against": "on","joincount": "on","label": "on"&#125;,"tcount": 0,"title": "习近平会见加拿大总督","url": "http://news.163.com/17/0713/20/CP8JPMSD000189FH.html","vote": 83&#125; 通过分析发现网易的跟帖数为cmtAgainst+cmtVote+rcount，因此只需解析此Json数据并将3值加起来 评论地址中有两个参数是从新闻详情页中获取，通过查看其网页源代码可以发现其具体值，类似以下结构1234567891011var config = &#123; "productKey" : "a2869674571f77b5a0867c3d71db5856", "docId" : "CP8JPMSD000189FH", "target" : document.getElementById("post_comment"), //Dom 容器 //展示的功能按钮：顶、踩、回复、收藏、举报、分享 "operators": ["up", "down", "reply", "share"], "isShowComments": isShowComments, //是否显示帖子列表 "hotSize": 3, //热门跟贴列表 展示 3 条 "newSize": 2, //最新跟贴列表 展示 2 条 "submitType": "commentPage" //新发帖子的展现形式：停留在当前页面(currentPage) | 跳转到跟贴详情页(commentPage) &#125;; 腾讯新闻新闻网址首页：http://news.qq.com/新闻详情页地址格式：http://[\S].qq.com/a/[0-9]{8}/[0-9].htm，比如 http://news.qq.com/a/20170712/037822.htm新闻时间格式：yyyy-MM-dd hh:mm评论数：动态获取 评论地址 http://coral.qq.com/article/+cmt_id+/comment?commentid=0&amp;reqnum=1&amp;tag=&amp;callback=mainComment&amp;_=1389623278900 ,比如 http://coral.qq.com/article/2026122514/comment?commentid=0&amp;reqnum=1&amp;tag=&amp;callback=mainComment&amp;_=1389623278900123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293mainComment(&#123;"errCode": 0,"data": &#123;"targetid": 2026122514,"display": 1,"total": 152,"reqnum": 1,"retnum": 1,"maxid": "6291427308406998878","first": "6291427308406998878","last": "6291427308406998878","hasnext": true,"commentid": [&#123;"id": "6291427308406998878","rootid": "0","targetid": 2026122514,"parent": "0","timeDifference": "5分钟前","time": 1499993159,"content": "坚信全中国各族人民在习总书记的带领下，终将实现我们中华民族伟大复兴的中国梦。","title": "","up": "0","rep": "0","type": "1","hotscale": "0","checktype": "2","checkstatus": "1","isdeleted": "0","tagself": "","taghost": "","source": "1","location": "","address": "","rank": "-1","custom": "","extend": &#123;"at": 0,"ut": 0,"ct": "","wt": 0&#125;,"orireplynum": "0","richtype": 0,"userid": "279853601","poke": 0,"abstract": "","thirdid": "","ispick": 0,"ishide": 0,"isauthor": 0,"replyuser": "","replyuserid": 0,"replyhwvip": 0,"replyhwlevel": 0,"replyhwannual": 0,"userinfo": &#123;"userid": "279853601","uidex": "ec365d5f07251791cb0ba9b8437c382bbd","nick": "捍卫","head": "http://q3.qlogo.cn/g?b=qq&amp;k=hMeU8ic3IdgXGwaQpQ6iaJjw&amp;s=40&amp;t=1499961600","gender": 1,"viptype": "0","mediaid": 0,"region": "新加坡::","thirdlogin": 0,"hwvip": 0,"hwlevel": 0,"hwannual": 0,"identity": "","wbuserinfo": [],"certinfo": "","remark": "","fnd": 0&#125;&#125;],"targetinfo": &#123;"orgcommentnum": "414","commentnum": "152","checkstatus": "0","checktype": "2","city": "","voteid": "","topicids": ""&#125;&#125;,"info": &#123;"time": 1499993452&#125;&#125;) 其中 total 即为评论总数 评论地址中有几个变量 cmt_id: 可以从新闻详情页中获取 12345document.domain = 'qq.com'; cmt_site = 'news'; cmt_id = 2026122514; cmt_is_group = 0; cmt_count_id = 'comment_count|comment_count2'; commentid: 表示评论内容Id reqnum：请求返回评论内容条数结合参数 commentid与reqnum，我们可以获取全部评论内容的Json格式数据，由于我们进仅需要获取评论总是，因此将commentid设置为0，reqnum设置为1新浪新闻新闻网址首页：http://news.sina.com.cn/新闻详情页地址格式：http://[\S].sina.com.cn/[\S]/[0-9]{4}-[0-9]{2}-[0-9]{2}/doc-[\S]*.shtml ，比如 http://finance.sina.com.cn/chanjing/gsnews/2017-07-14/doc-ifyiakwa4053811.shtml新闻时间格式：yyyy年MM月dd日hh:mm评论数：动态获取 评论地址：http://comment5.news.sina.com.cn/page/info?version=1&amp;format=js&amp;channel=+comment_channel+&amp;newsid=+comment_id ，比如 http://comment5.news.sina.com.cn/page/info?version=1&amp;format=js&amp;channel=cj&amp;newsid=comos-fyiakwa405381112345var data=&#123;"result": &#123;"status": &#123;"msg": "", "code": 0&#125;, "count": &#123;"qreply": 165, "total": 351, "show": 144&#125;,....&#125;&#125; 其中 total 即为评论总数 评论地址变量：可以从新闻详情页获取1&lt;meta name="sudameta" content="comment_channel:cj;comment_id:comos-fyiakwa4053811" /&gt; comment_channel comment_id搜狐新闻新闻网址首页：http://news.sohu.com/新闻详情页地址格式：http://[\S].sohu.com/[0-9]{8}/[\S].shtml ，比如：http://news.sohu.com/20170629/n499130144.shtml新闻时间格式：yyyy-MM-dd hh:mm:ss评论数：动态获取 评论地址：http://changyan.sohu.com/api/3/topic/liteload?callback=jQuery1709683075909326675_1500009616671&amp;client_id=cyqemw6s1&amp;topic_url=+url ，比如 http://changyan.sohu.com/api/3/topic/liteload?callback=jQuery1709683075909326675_1500009616671&amp;client_id=cyqemw6s1&amp;topic_url=http://news.sohu.com/20170629/n499130144.shtml 123jQuery1709683075909326675_1500009616671(&#123;"cmt_sum":43,"comments":[],"hideList":true,"hots":[],"mode":2,"participation_sum":32655,"source_id":"499130144","topic_category_id":"143746642|143746653|451178592|499130144","topic_id":3400490087&#125;); 评论地址变量：可以从新闻详情页获取topicurl的值即为新闻详情地址 participation_sum：参与人数 cmt_sum：评论数凤凰网新闻网址首页：http://www.ifeng.com/新闻详情页地址格式：http://[\S].ifeng.com/a/[0-9]{8}/[0-9]_0.shtml ，比如 http://news.ifeng.com/a/20170714/51431015_0.shtml新闻时间格式：yyyy年MM月dd日 hh:mm:ss评论数：动态获取 评论地址：http://comment.ifeng.com/get.php?doc_url=+commentUrl+&amp;format=js&amp;job=1&amp;callback=callbackGetFastCommentCount&amp;callback=callbackGetFastCommentCount ,比如 http://comment.ifeng.com/get.php?doc_url=http://news.ifeng.com/a/20170714/51431015_0.shtml&amp;format=js&amp;job=1&amp;callback=callbackGetFastCommentCount&amp;callback=callbackGetFastCommentCount 12(function()&#123;var commentJsonVarStr___=&#123;"count":151,"join_count":1329,"comments":[.....]&#125;&#125; 评论地址变量：可以从新闻详情页获取，commentUrl即为需要替换的变量 1234567891011// @TODO: 全局参数 define("detail", [], function () &#123; return &#123; "docName": "国防部回应“中国军机飞越宫古海峡”：习惯就好", "docUrl": "http://news.ifeng.com/a/20170714/51431015_0.shtml", "summary": '原标题：国防部新闻发言人任国强答记者问记者问：据媒体报道，日本防卫省统和幕僚监部13日发布消息称，中国空军6架轰炸机当日飞越宫古海峡，日方还公布了中国军机的照片。请问对此作何评论？国防部新闻局：这是一', "skey":"74cfb2", "commentUrl":"http://news.ifeng.com/a/20170714/51431015_0.shtml", "image": "http://p0.ifengimg.com/pmop/2017/0713/DF9496ED515EDD4A444FCAAFE95556374A73CF54_size164_w720_h572.png", "channelTitle": "资讯" &#125;; &#125;); count:评论数 join_count：参与人数 返回数据中包含评论内容，此处省略21CN新闻网址首页：http://www.21cn.com/新闻详情页地址格式：http://[\S].21cn.com/[\S]/[0-9]{4}/[0-9]{4}/[0-9]{2}/[0-9]*.shtml ，比如http://news.21cn.com/caiji/roll1/a/2017/0117/15/31896281.shtml新闻时间格式：yyyy-MM-dd hh:mm:ss评论数：动态获取 评论地址：http://review.21cn.com/review/list.do?jsoncallback=jQuery17206256071045321705_1499329382240&amp;operationId=0&amp;contentId=+contentId+&amp;pageNo=1&amp;pageSize=10&amp;sys=cms&amp;order=new&amp;_=1499329382281 ，比如 http://review.21cn.com/review/list.do?jsoncallback=jQuery17206256071045321705_1499329382240&amp;operationId=0&amp;contentId=31896281&amp;pageNo=1&amp;pageSize=10&amp;sys=cms&amp;order=new&amp;_=1499329382281 12345678jQuery17206256071045321705_1499329382240( &#123;"summary":&#123;"browseNum":0,"contentAbstract":"(闫旭)今年是香港回归20周年，福建省政协主席张昌平17日在福州表示，要积极参与闽籍社团纪念香港回归20周年等重大活动，加强与新生代乡亲的交流交往，进一步密切与港澳闽籍乡亲的联络联谊。福建省政协十一届五次会议当天在福州开幕，张昌平向大会作政协第十一届福建省委员会常务委员会工作报告。","contentId":31896281,"contentNickName":"先锋者","contentTime":"2017-01-17 15:49:46","contentTitle":"纪念香港回归20周年 闽政协主席促密切与港澳互融互通","contentUrl":"http://news.21cn.com/caiji/roll1/a/2017/0117/15/31896281.shtml","contentUserId":110425294,"displayReview":true,"displaySasFlag":0,"eggNum":0,"favoriteNum":0,"firstReviewTime":"2017-01-17 16:20:40","flowerNum":0,"forumFlag":1,"gradeAverageValue":0,"gradeFlag":1,"gradeItemMap":&#123;&#125;,"gradeNum":1,"gradeTotal":0,"gradeUserNum":0,"gradeValue":0,"hotDegree":6,"hotDegree4View":5,"lastGradeTime":"2017-02-12 16:03:46","lastReviewTime":"2017-03-24 14:15:12","lastReviewUserId":127017553,"operationId":9406,"operationName":"新闻频道","reviewAgainstNum":0,"reviewFlag":1,"reviewGroupSumId":0,"reviewNum":5,"reviewShareNum":0,"reviewSumNum":5,"reviewSupportNum":0,"reviewTaxisFlag":1,"reviewTopFlag":0,"reviewUserNum":5,"shareNum":0,"subscriptionNum":0,"sumId":23873292&#125;,"medalMap":&#123;&#125;,"userMedalMap":&#123;"127017441":[],"127017260":[],"127017097":[],"127017553":[],"127017164":[]&#125;,"pageTurn":&#123;"currentPage":1,"end":5,"firstPage":1,"nextPage":1,"page":1,"pageCount":1,"pageSize":10,"prevPage":1,"rowCount":5,"start":0&#125;,"list":[&#123;"againstNum":0,"audioId":0,"childList": 评论地址变量：可以从新闻详情页获取，替换contentId变量值 1234567891011ARTICLE_INFO = window.ARTICLE_INFO || &#123; site: strSite, sect_id: 101029, sect_name: '滚动热点', title: '纪念香港回归20周年 闽政协主席促密切与港澳互融互通', operaId: 9406, contentId: 31896281, typeId: 101029, article_url: 'http://news.21cn.com/caiji/roll1/a/2017/0117/15/31896281.shtml', pubtime: '2017-01-17 15:49:46' &#125; reviewSumNum: 评论数奥一网新闻网址首页：http://www.oeeee.com/新闻详情页地址格式：http://www.oeeee.com/html/[0-9]{6}/[0-9]{2}/[0-9]*.html ，比如 http://www.oeeee.com/html/201707/13/493459.html新闻时间格式：yyyy-MM-dd hh:mm阅读量：动态获取 阅读量获取地址：http://www.oeeee.com/api/readnum.php?s=/readnum/readnum/id/+id+/type/article/show/1&amp;_=1497687490644 ,比如 http://www.oeeee.com/api/readnum.php?s=/readnum/readnum/id/493459/type/article/show/1&amp;_=1497687490644 1206 阅读量地址变量：id，可以从详情页url获取，比如上面所提新闻id为493459 新浪微博由于新浪微博的数据获取需要登录，具体可参考使用Selenium登录新浪微博 获取博主sinaName关于keywords主题的报道数 url地址：https://weibo.cn/search/mblog/?keyword=keywords&amp;filter=hasv&amp;nick=sinaName&amp;starttime=starttime&amp;endtime=endtime ,比如 https://weibo.cn/search/mblog/?keyword=香港回归&amp;filter=hasv&amp;nick=人民网&amp;starttime=20170701&amp;endtime=20170712 地址中变量 keywords：关键词，即需要获取与关键词相关的信息 sinaName：新浪微博名，即需要获取哪个微博的相关信息 starttime：开始时间，格式为yyyyMMdd endtime：结束时间，格式为yyyyMMdd获取新浪微博详细信息 详情页可以获取报道更多详细信息，如转发数、点赞数、评论数，以及转发列表、评论列表以及点赞列表，比如 https://weibo.cn/repost/Fbk5Y54in?uid=2286908003&amp;rl=1 评论 转发 点赞]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>网页爬虫</tag>
        <tag>新闻</tag>
        <tag>网易新闻</tag>
        <tag>腾讯新闻</tag>
        <tag>凤凰网</tag>
        <tag>新浪新闻</tag>
        <tag>搜狐新闻</tag>
        <tag>新浪微博</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态规划之矩阵链乘法]]></title>
    <url>%2F2017%2F07%2F13%2Fdynamic-matrix-chain%2F</url>
    <content type="text"><![CDATA[矩阵链乘法问题描述给定一个 $n$ 个矩阵的序列(矩阵链) &lt;$A_1,A_2,…,A_n$&gt;,我们希望计算它们的乘积$$A_1A_2…A_n$$为了计算表达式，我们可以先用括号明确计算次序，然后利用标准的矩阵相乘算法进行计算。由于矩阵乘法满足结合律，因此任何加括号的方法都会得到相同的计算结果。我们称有如下性质的矩阵乘积链为完全括号话：它是单一矩阵，或者是两个完全括号化的矩阵乘积链的积，且已外加括号。例如，如果矩阵链为&lt;$A_1,A_2,A_3,A_4$&gt;,则共有 5 种完全括号化的矩阵乘积链：$$(A_1(A_2(A_3A_4))),(A_1(A_2A_3(A_4))),((A_1A_2)(A_3A_4)),((A_1A_2A_3)(A_4)),(((A_1A_2)A_3)A_4)$$两个矩阵 $A$ 和 $B$ 只有相容，即$A$的列数等于 $B$ 的行数时，才能相乘。如果 $A$ 是 $p \times q$ 的矩阵，$B$ 是 $q \times r$ 的矩阵那么乘积 $C$ 是 $p \times r$ 的矩阵。计算 $C$ 的代价为 $pqr$，我们使用标量乘法的次数来表示计算代价。矩阵链乘法问题：给定 $n$ 个矩阵的链&lt;$A_1,A_2,…,A_n$&gt;，矩阵 $A_i$ 的规模为 $p_{i-1} \times p_i (1\leq i \leq n )$，求完全括号化方案，使得计算乘积 $A_1A_2…A_n$所需的标量乘法次数最少。 动态规划求解最优解结构为了方便起见，我们用符号 $A_{i.j}(i \leq j)$ 表示$A_iA_{i+1}…A_j$ 乘积的结果矩阵。假设$A_iA_{i+1}…A_j$ 的最优括号化方案的分割点在 $A_k$ 和 $A_{k+1}$ 之间。那么，继续对”前缀”子链 $A_iA_{i+1}…A_k$，”后缀” $A_{k+1}A_{k+2}…A_j$ 进行括号化 (独立求解)。 递归定义令 $m[i,j]$ 表示计算矩阵 $A_{i.j}$ 所需标量乘法次数的最小值，原问题的最优解—–计算$A_{1..n}$ 所需的最低代价就是 $m[1,n]$。我们假设 $A_iA_{i+1}…A_j$ 的最优括号化方案的分割点在矩阵 $A_k$ 和 $A_{k+1}$ 之间，其中 $i \leq k &lt;j$。 那么，$m[i,j]$ 就等于计算 $A_{i.k}$ 和 $A_{k+1.j}$ 的代价加上两者相乘的代价的最小值：$$m[i,j]=m[i,k]+m[k+1,j]+p_{i-1}p_kp_j$$因此递归求解公式为if i==j$$m[i,j]=0$$else if i&lt;j$$m[i,j]=min\{m[i,k]+m[k+1,j]+p_{i-1}p_kp_j\},i \leq k&lt;j$$ 计算最优值12345678910111213141516171819202122public static void getMatrixChanOrder(int[] p) &#123; int n = p.length; long[][] dp = new long[n][n]; int[][] s = new int[n][n]; for (int i = 0; i &lt; n; i++) &#123; dp[i][i] = 0; &#125; for (int l = 2; l &lt; n; l++) &#123; for (int i = 1; i &lt;= n - l; i++) &#123; int j = i + l - 1; dp[i][j] = Long.MAX_VALUE; for (int k = i; k &lt; j; k++) &#123; long temp = dp[i][j]; dp[i][j] = Math.min(dp[i][j], dp[i][k] + dp[k + 1][j] + p[i - 1] * p[k] * p[j]); if(temp!=dp[i][j])&#123; s[i][j]=k; &#125; &#125; &#125; &#125; System.out.println(dp[1][n-1]+" "+s[1][n-1]);&#125; 假定矩阵 $A_i$ 的规模为 $p_{i-1} \times p_i,(i=1,2,…,n)$，输入的序列 $p=$&lt;$p_0,p_1,…,p_n$&gt;比如： 矩阵 $A_1$ $A_2$ $A_3$ $A_4$ $A_5$ $A_6$ 规模 $30 \times 35$ $35 \times 15$ $15 \times 5$ $5 \times 10$ $10 \times 20$ $20 \times 25$ 则1int[] p = &#123; 30, 35, 15, 5, 10, 20, 25 &#125;; 构造最优解12345678910public static void printOptimal(int[][] s,int i,int j)&#123; if(i==j)&#123; System.out.print("A"+i); &#125;else&#123; System.out.print("("); printOptimal(s,i,s[i][j]); printOptimal(s,s[i][j]+1,j); System.out.print(")"); &#125;&#125; 完整代码以及相应示例图见 https://github.com/lemon2013/algorithm/blob/master/MatrixChain.java]]></content>
      <categories>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>矩阵链乘法</tag>
        <tag>算法</tag>
        <tag>算法导论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深圳市媒体传播效力评估系统用户手册]]></title>
    <url>%2F2017%2F07%2F09%2Fuser-manual%2F</url>
    <content type="text"><![CDATA[首页 显示数据源数目 数据采集模板：包括网站论坛、新浪微博、百度新闻以及百度搜索四大类 数据分析：表格分析，获取各大新闻网站、新浪微博等相关新闻的评论量、阅读量、点赞量以及转发量等信息 折线图：按天统计三大新闻类别（网站论坛、新浪微博、百度新闻）相关主题报道数，以折线图形式展示 饼状图：统计三个新闻类别在特定时间的新闻发布量，以饼状图展示 数据获取网站论坛 数据源选取：包括大粤网、奥一网、天涯社区以及深圳新闻网 关键词：用户输入待获取新闻主题关键词 时间范围选取： 点击抓取数据按钮即可爬取相关内容得到临时列表，支持数据的查询、导出以及二次筛选，通过点击提交按钮得到最终数据，可在数据分析列表中查看。新浪微博 操作方法跟网站论坛模块类似百度新闻 固定数据源，一共12个主要新闻网站：1.网易 2.新浪 3.21CN 4.腾讯 5.界面新闻 6.南方周末 7.凤凰网 8.澎湃新闻 9.搜狐 10.新华 11.人民网 12.南方网 其他操作类似百度关键词搜索 登录百度账号 如搜索“粤港澳大湾区”数据分析新闻列表包含所有已上传新闻报道信息，支持删除、查找以及导出等功能条形图 百度新闻：统计百度新闻关于特定新闻主题的报道数，以条形图展示。 新浪微博：统计各官微对特定新闻主题的报道数，以条形图展示。 网站论坛：统计各大网站对特定新闻主题的报道数，以条形图展示。折线图 百度新闻：按天统计百度新闻对特定新闻主题的报道数，以折线图展示。 新浪微博：按天统计新浪微博对特定新闻主题的报道数，以折线图展示。 网站论坛：按天统计网站论坛对特定新闻主题的报道数，以折线图展示。数据源列表新闻数据爬取源]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>项目</tag>
        <tag>网页爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态规划之钢条切割]]></title>
    <url>%2F2017%2F05%2F31%2Fdynamic-programming%2F</url>
    <content type="text"><![CDATA[动态规划简介(算法导论)动态规划方法通常用来求解最优化问题。这类问题可以有很多可行解，每个解都有一个值，我们希望寻找具有最优值(最小值或最大值)的解。我们称这样的解为问题的一个最优解，但是不是唯一的最优解，因为可能有多个解都能达到最优值。 我们通常按如下4个步骤来设计一个动态规划算法： 刻画一个最优解的结构特征。 递归地定义最优解的值。 计算最优解的值，通常采用自底向上的方法。 利用计算出的信息构造一个最优解。 钢条切割问题描述给定一段长度为 $n$ 英寸的钢条和一个价格表 $p_i (i=1,2,…,n)$， 求切割钢条方案，使得销售收益 $r_n$ 最大。 长度 $i$ 1 2 3 4 5 6 7 8 9 10 价格 $p_i$ 1 5 8 9 10 17 17 20 24 30 表1.1 钢条价格样例。每段长度为 $i$ 英寸的钢条为公司带来 $p_i$ 美元的收益 动态规划求解长度为 $n$ 英寸的钢条共有 $2^{n-1}$ 种不同的切割方案。在距离钢条左端 $i(i=1,2,…,n-1)$ 英寸处，我们可以选择切割或者不切割。为了方便描述，我们用加法符号表示切割方案，比如7=2+2+3表示将长度为7英寸的钢条切割为三段(长度为2、2、3)。如果一个最优解将钢条切割 $k$ 段($1&lt;=k&lt;=n$),那么最有切割方案，可以用如下表示方式$$n=i_1+i_2+…+i_k$$ ,将钢条切割为长度为 $i_1,i_2,…,i_k$ 的小段，得到最大利益表示如下$$r_n=p_{i_1}+p_{i_2}+…+p_{i_k}$$ 最优解结构为了求解规模为 $n$ 的原问题，我们可以将其分解为规模更小的子问题求解。如果问题的最优解由相关子问题的最优解组合得到，我们则称此问题满足最优子结构。在钢条切割问题上，我们完成钢条首次切割后,可以将两段钢条看成两个独立的钢条切割实例。通过组合两个子问题的最优解，并在所有可能中取收益最大的组合方案得到最优解。 递归定义将钢条从左边切割长度为 $i$ 的一段，右边剩下长度为 $n-1$ ,假定左边钢条不再切割，只需对右边钢条继续切割，使用递归的思想求解。可得到以下递归版本定义$$r_n = \max(p_i+r_{n-i}),1\leq i \leq n.$$123456789101112public static long getCutRod(long[] p, int n) &#123; if (n == 0) return 0; long q = Long.MIN_VALUE; for (int i = 1; i &lt;= n; i++) &#123; if(i==11)&#123; System.out.println(i); &#125; q = Math.max(q, p[i] + getCutRod(p, n - i)); &#125; return q; &#125; 其中数组p表示价格数组，数组大小为n+1，初始化如下if(n&lt;=10),数组大小定义为11，初始化如下p[0] = 0;p[1] = 1;p[2] = 5;p[3] = 8;p[4] = 9;p[5] = 10;p[6] = 17;p[7] = 17;p[8] = 20;p[9] = 24;p[10] = 30;其他情况，数组大小根据输入的n值大小动态确定，前11个值初始化为上面所提，其他为默认值0但是该版本的运行时间随着n的增加呈指数增长，后面会给出对比曲线。 计算最优值我们采用自底向上的动态规划的方法得到更有效的算法。1234567891011121314151617public static long getCutRodDP(long[] p, int n) &#123; long[] dp = new long[n + 1]; int[] length = new int[n + 1]; dp[0] = 0; for (int i = 1; i &lt;= n; i++) &#123; long q = Long.MIN_VALUE; for (int j = 1; j &lt;= i; j++) &#123; long temp = q; q = j &lt; p.length ? Math.max(q, p[j] + dp[i - j]) : Math.max(q, dp[j] + dp[i - j]); if (temp &lt; q) &#123; length[i] = j; &#125; &#125; dp[i] = q; &#125; return dp[n]; &#125; p表示价格表，如本文中应该为p的数组大小应该为11。动态规划版本与递归版本时间消耗对比如下,可以明显看到递归版本的时间呈指数增长当钢条长度增长到35时，鄙人渣渣电脑就跑不动了，以下表示动态规划算法版本的时间消耗表 长度 200 400 600 800 1000 1500 2000 4000 6000 10000 耗时(ms) 1 3 4 6 7 11 13 29 54 133 构造最优解前面我们进给出最优解的值，并没给出具体切割方案1234567891011121314151617181920212223242526public static String getCutRodDP(long[] p, int n) &#123; long[] dp = new long[n + 1]; int[] length = new int[n + 1]; dp[0] = 0; for (int i = 1; i &lt;= n; i++) &#123; long q = Long.MIN_VALUE; for (int j = 1; j &lt;= i; j++) &#123; long temp = q; q = j &lt; p.length ? Math.max(q, p[j] + dp[i - j]) : Math.max(q, dp[j] + dp[i - j]); if (temp &lt; q) &#123; length[i] = j; &#125; &#125; dp[i] = q; &#125; //构造最优解 int temp = n; StringBuffer str = new StringBuffer("r" + n + "=" + dp[n] + "," + "切割方案" + n + "="); while (temp &gt; 0) &#123; str.append(length[temp]); str.append("+"); temp = temp - length[temp]; &#125; str.delete(str.length() - 1, str.length()); return str.toString(); &#125; 测试r11=31,切割方案11=1+10，r15=43,切割方案15=2+3+10，r33=98,切割方案33=3+10+10+10 完整代码以及相应示例图见 https://github.com/lemon2013/algorithm/blob/master/src/com/pty/graph/Cutrod.java]]></content>
      <categories>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>算法导论</tag>
        <tag>钢条切割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图搜索算法]]></title>
    <url>%2F2017%2F05%2F27%2Fgraph-search%2F</url>
    <content type="text"><![CDATA[图的表示所谓的图$G=(V,E)$，由顶点(Vertex) $V$ 和边(Edges) $E$ 组成。可以用两种标准方式来表示： 邻接链表 邻接矩阵 根据图是否有向，可以将图分为有向图和无向图。 邻接链表邻接链表由 $|V|$ 条链表构成，即每个顶点 $V_i \in V$有一条链表，链表中存储该顶点的相邻顶点。一般来说，邻接链表更适合表示稀疏图(边的条数$|E|$ 远远小于$|V|^2$的图)。如上图所示，图C为无向图A($G=(V,E),|V|=5,|E|=7$)的邻接链表表示，共有5条链表，且所有邻接链表的长度之和等于 $2|E|$, 即14。图D为有向图B($G=(V,E),|V|=5,|E|=7$)得邻接链表表示，邻接链表的长度之和等于 $2|E|$ 。不论是有向图还是无向图均需要的存储空间为 $\Theta(V+E)$。 邻接矩阵对于邻接链表而言，存在一个明显的不足就是无法快速判断边$(u,v)$ 是否为图中的边，而邻接矩阵恰恰克服了这一缺陷。邻接矩阵，对于图$G$而言，其邻接矩阵由 $|V|\times|V|$的矩阵 $A=(a_{ij})$ 表示, 并满足以下关系: $a_{ij}=1 , 如果(i,j) \in E$ $a_{ij}=0 , 其他$ 图E、图F分别给出了无向图与有向图的邻接矩阵表示，其空间需求皆为$\Theta(V^2)$。与邻接链表相反，邻接矩阵更适合表示稠密图(边的条数$|E|$ 接近 $|V|^2$的图)。 图的遍历广度优先搜索(BFS)给定图$G = (V,E)$以及源点 $S$, 通过调用广度优先算法可以发现从源点到达的所有顶点，同时生产一颗 “广度优先搜索树”。这里我们将借助一个先进先出(FIFO)的队列 $Queue$ 实现算法。下面给出该算法的文字描述： 初始化所有顶点，将顶点标记为未访问。 将顶点 $S$ 存入队列 $Queue$ 中。 如果队列 $Queue$ 非空，进入下一步，否则退出。 出队列 $DeQueue$ ，得到顶点 $u$，如果 $u$ 未被访问，进入下一步，否则回到步骤3。 将 $u$ 标记为已访问，并将顶点 $u$ 的所有满足条件(1.未访问，2.队列中不存在该结点)的邻接结点存入$Queue$ 中,。 打印顶点 $u$，跳转到步骤3. 为了便于理解，我们结合图A中表示的无向图，源点为A，给出BFS的执行过程： . 如图(a)所示，我们初始化所有顶点状态未访问(用绿色表示)，将源点$A$存入队列 $Queue$。 . 如图(b)所示，弹出$A$并将其邻接结点$B,E$加入队列中,输入$A$。 . 如图(c)所示，弹出队列头部元素$B$，将其满足条件的邻接结点 $C,D$ 存入队列，输出 $B$。 . 如图(d)所示，弹出队列头部元素$E$，其邻接结点均已入队列，输出 $E$。 . 如图(e)所示，弹出队列头部元素$C$，其邻接结点均已入队列，输出 $C$。 . 如图(f)所示，弹出队列头部元素$D$，其邻接结点均已入队列，输出 $D$。 . 队列 $Queue$ 为空，终止算法。最终得到输出数列 $A,B,E,C,D$，其广度优先搜索树如下图G所示： 深度优先搜索(DFS)深度优先搜索总是对最近才发现的结点 $v$ 的出发边遍历直到结点的所有出发边均被发现，然后再“回溯”到 $v$ 的前驱结点来搜索其出发边。这里我们借助栈 $Stack$ 的思想来描述算法。下面给出该算法的文字描述： 初始化所有顶点，将顶点标记为未访问。 将出发点 $S$ 压入栈 $Stack$ 中。 如果栈非空，进入下一步，否则退出结束算法。 弹出栈顶结点 $u$，如果 $u$ 未被访问，进入下一步，否则回到步骤3。 将 $u$ 标记为已访问，并将顶点 $u$ 的所有满足条件(1.未访问，2.栈中不存在该结点)的邻接结点存入$Stack$ 中。 打印结点 $u$，跳转到步骤3. 为了更加直观的描述DFS的思路，我们结合图B 与源点 $A$ ，给出算法DFS的执行过程： 图的实现(Java)邻接链表实现图的邻接链表表示我们知道描述一张图有两个最基本的元素结点 $V$ 和边 $E$，在这里我们首先定义一个类Graph.java来表示图,将Vertex.java作为其内部类表示结点，实现代码如下。123456789101112131415161718192021222324252627282930313233343536package com.pty.graph;import java.util.ArrayList;import java.util.LinkedHashMap;public class Graph&lt;T&gt; &#123; private LinkedHashMap&lt;T, Vertex&gt; vertexs;// 使用LinkedHashMap存放图中的结点，结点标识T作为key能快速查找图中是否包含某结点 private LinkedHashMap&lt;T, ArrayList&lt;Vertex&gt;&gt; adjList; // 采用邻接链表的方式表示图，每个结点T对应于一个ArrayList链表，存放其相邻结点。 private int numOfVertex; // 图的结点数 private int numOfEdge; // 图的边数 private boolean directed; // 是否为有向图 public Graph(boolean directed) &#123; vertexs = new LinkedHashMap&lt;T, Vertex&gt;(); numOfVertex = 0; adjList = new LinkedHashMap&lt;T, ArrayList&lt;Vertex&gt;&gt;(); this.directed = directed; &#125; //省略相应的get、set方法 /** * 内部类，表示结点 * * @author john * */ public class Vertex &#123; private T lable; // 标识结点，比如A0，A1，A2,...或者1，2，3，... private boolean visited; // 标识结点是否被访问 public Vertex(T tag, double score) &#123; lable = tag; visited = false; &#125; //省略相应的get、set方法 &#125;&#125; 图的基本操作添加新的结点每新增一个结点，将会创建一个ArrayList列表，用于存放新增结点的邻接结点。12345public void insertVertex(T tag) &#123; vertexs.put(tag, new Vertex(tag)); //新增结点 tag，如果定点中存在该结点将会被替换最新的 adjList.put(tag, new ArrayList&lt;Vertex&gt;()); //每新增一个结点，将会创建一个ArrayList列表，用于存放新增结点的邻接结点。 numOfVertex++; //结点数自增&#125; 添加新的边这里暂时不考虑边的权值123456789101112public boolean addEdges(T start, T end) &#123; if (vertexs.containsKey(start) &amp;&amp; vertexs.containsKey(end)) &#123; // 判断输入起始点是否合法 adjList.get(start).add(vertexs.get(end)); // 首先获取结点start的链表，然后将其邻接结点end加入其中 if (!directed) &#123; //如果是无向图，则添加结束点到开始点的边 adjList.get(end).add(vertexs.get(start)); &#125; &#125; else &#123; System.out.println("输入结点不合法"); return false; &#125; return true;&#125; 测试打印图的链表结构12345678910111213141516171819202122232425262728/** * 打印图的邻接链表 */public void displayGraph() &#123; System.out.println("邻接链表表示如下："); Iterator&lt;Map.Entry&lt;T, ArrayList&lt;Vertex&gt;&gt;&gt; iterator = adjList.entrySet().iterator(); while (iterator.hasNext()) &#123; Map.Entry&lt;T, ArrayList&lt;Vertex&gt;&gt; element = iterator.next(); System.out.print(element.getKey() + ":"); displayList(element.getValue()); &#125;&#125;/** * 打印链表ArrayList * * @param list */public void displayList(ArrayList&lt;Vertex&gt; list) &#123; int size = list.size(); for (int i = 0; i &lt; size; i++) &#123; System.out.print(list.get(i).getLable()); if (i &lt; size - 1) &#123; System.out.print("--&gt;"); &#125; &#125; System.out.println();&#125; 测试数据(无向图A)1234567891011121314151617181920package com.pty.graph;public class Test &#123; public static void main(String[] args) &#123; Graph&lt;String&gt; graph = new Graph&lt;String&gt;(false); graph.insertVertex("A"); graph.insertVertex("B"); graph.insertVertex("C"); graph.insertVertex("D"); graph.insertVertex("E"); graph.addEdges("A", "B"); graph.addEdges("A", "E"); graph.addEdges("B", "C"); graph.addEdges("B", "D"); graph.addEdges("B", "E"); graph.addEdges("C", "D"); graph.addEdges("D", "E"); graph.displayGraph(); &#125;&#125; 控制台输出结果：A:B–&gt;EB:A–&gt;C–&gt;D–&gt;EC:B–&gt;DD:B–&gt;C–&gt;EE:A–&gt;B–&gt;D 图的相关算法BFS实现1234567891011121314151617181920212223242526272829303132public void BFS(T start) &#123; ArrayList&lt;T&gt; list = new ArrayList&lt;T&gt;(); // 存放遍历结点数列 LinkedList&lt;Vertex&gt; tempList = new LinkedList&lt;Vertex&gt;(); // 辅助BFS的队列 initialize(); //初始化所有结点访问属性为false tempList.add(vertexs.get(start)); while (!tempList.isEmpty()) &#123; Vertex node = tempList.poll(); // 弹出队列头 if (!node.isVisited()) &#123; node.setVisited(true); ArrayList&lt;Vertex&gt; neighbor = adjList.get(node.getLable()); // 获取结点node的邻接表 int size = neighbor.size(); for (int i = 0; i &lt; size; i++) &#123; if ((!neighbor.get(i).isVisited()) &amp;&amp; (!tempList.contains(neighbor.get(i)))) &#123; tempList.offer(neighbor.get(i)); // 将未被访问且不包含在辅助BFS队列中的结点存入队列 &#125; &#125; list.add(node.getLable()); &#125; &#125; /** * 输出遍历序列 */ System.out.print("广度优先："); int size = list.size(); for (int i = 0; i &lt; size; i++) &#123; System.out.print(list.get(i)); if (i &lt; size - 1) &#123; System.out.print("--&gt;"); &#125; &#125; System.out.println();&#125; 控制台输出图A的 广度优先：A–&gt;B–&gt;E–&gt;C–&gt;D DFS实现12345678910111213141516171819202122232425262728public void DFS(T start) &#123; ArrayList&lt;T&gt; list = new ArrayList&lt;T&gt;(); // 存放遍历结点数列 Stack&lt;Vertex&gt; stack = new Stack&lt;Vertex&gt;(); // 辅助DFS栈 initialize(); // 初始化所有结点访问属性为false stack.push(vertexs.get(start)); while (!stack.isEmpty()) &#123; Vertex node = stack.pop(); //弹出栈顶元素 if (!node.isVisited()) &#123; node.setVisited(true); ArrayList&lt;Vertex&gt; neighbor = adjList.get(node.getLable()); // 获取结点node的邻接表 int size = neighbor.size(); for (int i = 0; i &lt; size; i++) &#123; if ((!neighbor.get(i).isVisited()) &amp;&amp; (!stack.contains(neighbor.get(i)))) // 将未被访问且不包含在辅助DFS栈中的结点压入栈 stack.push(neighbor.get(i)); &#125; list.add(node.getLable()); &#125; &#125; System.out.print("深度优先："); int size = list.size(); for (int i = 0; i &lt; size; i++) &#123; System.out.print(list.get(i)); if (i &lt; size - 1) &#123; System.out.print("--&gt;"); &#125; &#125; System.out.println();&#125; 控制台输出图A的 深度优先：A–&gt;E–&gt;D–&gt;C–&gt;B 附件完整代码以及相应示例图见 https://github.com/lemon2013/algorithm]]></content>
      <categories>
        <category>图论</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>广度优先搜索</tag>
        <tag>深度优先搜索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 常用操作]]></title>
    <url>%2F2016%2F11%2F23%2FLinux-foundation%2F</url>
    <content type="text"><![CDATA[查看Linux 版本12[root@localhost ~]# cat /etc/redhat-releaseCentOS Linux release 7.2.1511 (Core) Filesystem Hierarchy Standard（FHS）Linux目录配置依据 / （root, 根目录）：与开机系统有关；/usr （unix software resource）：与软件安装/执行有关；/var （variable）：与系统运行过程有关。 参考鸟哥的私房菜（部分）{鸟哥的私房菜(第四版)} 目录 存放内容 /bin 系统有很多放置可执行文件的目录，但/bin比较特殊。因为/bin放置的是在单人维护模式下还能够被操作的指令。在/bin下面的指令可以被root与一般帐号所使用，主要有：cat, chmod, chown, date, mv, mkdir, cp, bash等等常用的指令。 /boot 这个目录主要在放置开机会使用到的文件，包括Linux核心文件以及开机菜单与开机所需配置文件等等。 Linux kernel常用的文件名为：vmlinuz，如果使用的是grub2这个开机管理程序， 则还会存在/boot/grub2/这个目录喔！ /dev 在Linux系统上，任何设备与周边设备都是以文件的型态存在于这个目录当中的。 你只要通过存取这个目录下面的某个文件，就等于存取某个设备啰～ 比要重要的文件有/dev/null, /dev/zero, /dev/tty, /dev/loop, /dev/sd等等 /etc 系统主要的配置文件几乎都放置在这个目录内，例如人员的帐号密码档、各种服务的启始档等等。一般来说，这个目录下的各文件属性是可以让一般使用者查阅的， 但是只有root有权力修改。FHS建议不要放置可可执行文件（binary）在这个目录中喔。比较重要的文件有： /etc/modprobe.d/,/etc/passwd, /etc/fstab, /etc/issue 等等。另外 FHS 还规范几个重要的目录最好要存在 /etc/ 目录下喔：/etc/opt（必要）：这个目录在放置第三方协力软件 /opt 的相关配置文件 /etc/X11/（建议）：与 X Window 有关的各种配置文件都在这里，尤其是 xorg.conf 这个 X Server 的配置文件。 /etc/sgml/（建议）：与 SGML 格式有关的各项配置文件 /etc/xml/（建议）：与 XML格式有关的各项配置文件 /media media是“媒体”的英文，顾名思义，这个/media下面放置的就是可移除的设备啦！ 包括软盘、光盘、DVD等等设备都暂时挂载于此。常见的文件名有：/media/floppy, /media/cdrom等等。 /mnt 如果你想要暂时挂载某些额外的设备，一般建议你可以放置到这个目录中。在古早时候，这个目录的用途与/media相同啦！只是有了/media之后，这个目录就用来暂时挂载用了。 /opt 这个是给第三方协力软件放置的目录。什么是第三方协力软件啊？ 举例来说，KDE这个桌面管理系统是一个独立的计划，不过他可以安装到Linux系统中，因此KDE的软件就建议放置到此目录下了。 另外，如果你想要自行安装额外的软件（非原本的distribution提供的），那么也能够将你的软件安装到这里来。 不过，以前的Linux系统中，我们还是习惯放置在/usr/local目录下呢！ /run 早期的 FHS 规定系统开机后所产生的各项信息应该要放置到 /var/run 目录下，新版的 FHS 则规范到 /run 下面。 由于 /run 可以使用内存来仿真，因此性能上会好很多！ 查看Linux 核心版本、位数123456[root@localhost bin]# uname -a #全部Linux localhost.localdomain 3.10.0-327.36.1.el7.x86_64 #1 SMP Sun Sep 18 13:04:29 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux[root@localhost bin]# uname -m #操作位x86_64[root@localhost bin]# uname -r #核心版本3.10.0-327.36.1.el7.x86_64 更改文件属性 chgrp ：改变文件所属群组,存放于/etc/groupchown ：改变文件拥有者, 账号/etc/passwdchmod ：改变文件的权限, SUID, SGID, SBIT等等的特性 复制操作cp -a 整个数据复制完全一样，包括创建时间，如果是备份的是注意权限问题cp 创建时间，部分属性可能会发生改变cp -r 复制目录cp -l 实体链接cp -s 捷径链接cp -u 这个 -u 的特性，是在目标文件与来源文件有差异时，才会复制的。可以用来备份 文件或文件名的搜索whereis123456789[root@ceph001 ~]# whereis [-bmsu] 文件或目录名选项与参数：-l :可以列出 whereis 会去查询的几个主要目录而已-b :只找 binary 格式的文件-m :只找在说明文档 manual 路径下的文件-s :只找 source 来源文件-u :搜寻不在上述三个项目当中的其他特殊文件[root@ceph001 mlocate]# whereis cephceph: /usr/bin/ceph /usr/lib64/ceph /etc/ceph /usr/libexec/ceph /usr/share/ceph /usr/share/man/man8/ceph.8.gz locate根据/var/lib/mlocate/中的数据库文件进行搜索，手动更新时，updatedb 指令会去读取 /etc/updatedb.conf 这个配置文件的设置，然后再去硬盘里面进行搜寻文件名的动作， 最后就更新整个数据库文件12345678910111213[root@ceph001 ~]# locate [-ir] keyword选项与参数：-i ：忽略大小写的差异；-c ：不输出文件名，仅计算找到的文件数量-l ：仅输出几行的意思，例如输出五行则是 -l 5-S ：输出 locate 所使用的数据库文件的相关信息，包括该数据库纪录的文件/目录数量等-r ：后面可接正则表达式的显示方式[root@ceph001 mlocate]# locate ceph.conf/etc/ceph/ceph.conf/root/cluster/ceph.conf/usr/share/doc/ceph/sample.ceph.conf[root@ceph001 mlocate]# updatedb #手动更新数据文件 find1[root@ceph001 ~]# find [PATH] [option] [action] 选项与参数：1. 与时间有关的选项：共有 -atime, -ctime 与 -mtime ，以 -mtime 说明-mtime n ：n 为数字，意义为在 n 天之前的“一天之内”被更动过内容的文件；-mtime +n ：列出在 n 天之前（不含 n 天本身）被更动过内容的文件文件名；-mtime -n ：列出在 n 天之内（含 n 天本身）被更动过内容的文件文件名。-newer file ：file 为一个存在的文件，列出比 file 还要新的文件文件名 2. 与使用者或群组名称有关的参数：-uid n ：n 为数字，这个数字是使用者的帐号 ID，亦即 UID ，这个 UID 是记录在/etc/passwd 里面与帐号名称对应的数字。这方面我们会在第四篇介绍。-gid n ：n 为数字，这个数字是群组名称的 ID，亦即 GID，这个 GID 记录在/etc/group，相关的介绍我们会第四篇说明～-user name ：name 为使用者帐号名称喔！例如 dmtsai-group name：name 为群组名称喔，例如 users ；-nouser ：寻找文件的拥有者不存在 /etc/passwd 的人！-nogroup ：寻找文件的拥有群组不存在于 /etc/group 的文件！当你自行安装软件时，很可能该软件的属性当中并没有文件拥有者，这是可能的！在这个时候，就可以使用 -nouser 与 -nogroup 搜寻。 3. 与文件权限及名称有关的参数：-name filename：搜寻文件名称为 filename 的文件；-size [+-]SIZE：搜寻比 SIZE 还要大（+）或小（-）的文件。这个 SIZE 的规格有：c: 代表 Byte， k: 代表 1024Bytes。所以，要找比 50KB还要大的文件，就是“ -size +50k ”-type TYPE ：搜寻文件的类型为 TYPE 的，类型主要有：一般正规文件 （f）, 设备文件 （b, c）,目录 （d）, 链接文件 （l）, socket （s）, 及 FIFO （p） 等属性。-perm mode ：搜寻文件权限“刚好等于” mode 的文件，这个 mode 为类似 chmod的属性值，举例来说， -rwsr-xr-x 的属性为 4755 ！-perm -mode ：搜寻文件权限“必须要全部囊括 mode 的权限”的文件，举例来说，我们要搜寻 -rwxr–r– ，亦即 0744 的文件，使用 -perm -0744，当一个文件的权限为 -rwsr-xr-x ，亦即 4755 时，也会被列出来，因为 -rwsr-xr-x 的属性已经囊括了 -rwxr–r– 的属性了。-perm /mode ：搜寻文件权限“包含任一 mode 的权限”的文件，举例来说，我们搜寻-rwxr-xr-x ，亦即 -perm /755 时，但一个文件属性为 -rw——-也会被列出来，因为他有 -rw…. 的属性存在！ 4. 额外可进行的动作：-exec command ：command 为其他指令，-exec 后面可再接额外的指令来处理搜寻到的结果。-print ：将结果打印到屏幕上，这个动作是默认动作！ 当前linux支持的文件系统查看当前linux支持的文件系统12345678910111213141516171819202122232425262728[ceph@lemon ~]$ ls -l /lib/modules/$(uname -r)/kernel/fstotal 52-rw-r--r--. 1 root root 21853 Sep 18 21:46 binfmt_misc.kodrwxr-xr-x. 2 root root 21 Sep 25 17:21 btrfsdrwxr-xr-x. 2 root root 26 Sep 25 17:21 cachefilesdrwxr-xr-x. 2 root root 20 Sep 25 17:21 cephdrwxr-xr-x. 2 root root 20 Sep 25 17:21 cifsdrwxr-xr-x. 2 root root 22 Sep 25 17:21 cramfsdrwxr-xr-x. 2 root root 19 Sep 25 17:21 dlmdrwxr-xr-x. 2 root root 22 Sep 25 17:21 exofsdrwxr-xr-x. 2 root root 20 Sep 25 17:21 ext4drwxr-xr-x. 2 root root 48 Sep 25 17:21 fatdrwxr-xr-x. 2 root root 23 Sep 25 17:21 fscachedrwxr-xr-x. 2 root root 34 Sep 25 17:21 fusedrwxr-xr-x. 2 root root 20 Sep 25 17:21 gfs2drwxr-xr-x. 2 root root 21 Sep 25 17:21 isofsdrwxr-xr-x. 2 root root 20 Sep 25 17:21 jbd2drwxr-xr-x. 2 root root 21 Sep 25 17:21 lockd-rw-r--r--. 1 root root 19597 Sep 18 21:46 mbcache.kodrwxr-xr-x. 6 root root 4096 Sep 25 17:21 nfsdrwxr-xr-x. 2 root root 38 Sep 25 17:21 nfs_commondrwxr-xr-x. 2 root root 20 Sep 25 17:21 nfsddrwxr-xr-x. 2 root root 4096 Sep 25 17:21 nlsdrwxr-xr-x. 2 root root 23 Sep 25 17:21 overlayfsdrwxr-xr-x. 2 root root 23 Sep 25 17:21 pstoredrwxr-xr-x. 2 root root 24 Sep 25 17:21 squashfsdrwxr-xr-x. 2 root root 19 Sep 25 17:21 udfdrwxr-xr-x. 2 root root 19 Sep 25 17:21 xfs 查看系统已经载入到内存中支持的文件系统123456789101112131415161718192021222324[ceph@lemon ~]$ cat /proc/filesystemsnodev sysfsnodev rootfsnodev bdevnodev procnodev cgroupnodev cpusetnodev tmpfsnodev devtmpfsnodev debugfsnodev securityfsnodev sockfsnodev pipefsnodev anon_inodefsnodev configfsnodev devptsnodev ramfsnodev hugetlbfsnodev autofsnodev pstorenodev mqueuenodev selinuxfs xfsnodev rpc_pipefsnodev nfsd 查看磁盘与目录容量 df：列出文件系统的整体磁盘使用量 du：评估文件系统的磁盘使用量（常用在推估目录所占容量） df [-ahikHTm] [目录或文件名]选项与参数：-a ：列出所有的文件系统，包括系统特有的 /proc 等文件系统；-k ：以 KBytes 的容量显示各文件系统；-m ：以 MBytes 的容量显示各文件系统；-h ：以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示；-H ：以 M=1000K 取代 M=1024K 的进位方式；-T ：连同该 partition 的 filesystem 名称 （例如 xfs） 也列出；-i ：不用磁盘容量，而以 inode 的数量来显示 du [-ahskm] 文件或目录名称选项与参数：-a ：列出所有的文件与目录容量，因为默认仅统计目录下面的文件量而已。-h ：以人们较易读的容量格式 （G/M） 显示；-s ：列出总量而已，而不列出每个各别的目录占用容量；-S ：不包括子目录下的总计，与 -s 有点差别。-k ：以 KBytes 列出容量显示；-m ：以 MBytes 列出容量显示； 列出系统上所有磁盘列表 lsblk [-dfimpt] [device]选项与参数：-d ：仅列出磁盘本身，并不会列出该磁盘的分区数据-f ：同时列出该磁盘内的文件系统名称-i ：使用 ASCII 的线段输出，不要使用复杂的编码 （再某些环境下很有用）-m ：同时输出该设备在 /dev 下面的权限数据 （rwx 的数据）-p ：列出该设备的完整文件名！而不是仅列出最后的名字而已。-t ：列出该磁盘设备的详细数据，包括磁盘伫列机制、预读写的数据量大小等 查看设备的UUID(universally unique identifier,全域单一识别码)123456[root@lemon ~]# blkid #没行代表一个文件系统/dev/sda1: UUID="1649b929-1e62-46ef-aafe-dacb17e37ce9" TYPE="xfs" /dev/sda2: UUID="85mF3n-EX76-Vw0A-N4fI-5H91-Eaqq-UFVUD3" TYPE="LVM2_member" /dev/mapper/centos-root: UUID="6afcb5de-f93f-4b20-b31a-4c90de872e65" TYPE="xfs" /dev/mapper/centos-swap: UUID="f6f14cdb-481c-4385-8372-96ee47214f2a" TYPE="swap" /dev/mapper/centos-home: UUID="22549e34-e3a5-4bae-96d3-702cbdcc3487" TYPE="xfs" parted 列出磁盘的分区表类型与分区信息parted device_name print12345678910[root@lemon ~]# parted /dev/sda printModel: ATA ST3320613AS (scsi)Disk /dev/sda: 320GBSector size (logical/physical): 512B/512BPartition Table: msdosDisk Flags: Number Start End Size Type File system Flags 1 1049kB 525MB 524MB primary xfs boot 2 525MB 320GB 320GB primary lvm 挂载与卸载mount [-t 文件系统] 设备文件名 挂载点选项与参数：-a ：依照配置文件 /etc/fstab 的数据将所有未挂载的磁盘都挂载上来-l ：单纯的输入 mount 会显示目前挂载的信息。加上 -l 可增列 Label 名称！-t ：可以加上文件系统种类来指定欲挂载的类型。常见的 Linux 支持类型有：xfs, ext3, ext4,reiserfs, vfat, iso9660（光盘格式）, nfs, cifs, smbfs （后三种为网络文件系统类型）-n ：在默认的情况下，系统会将实际挂载的情况实时写入 /etc/mtab 中，以利其他程序的运行。但在某些情况下（例如单人维护模式）为了避免问题会刻意不写入。此时就得要使用 -n 选项。-o ：后面可以接一些挂载时额外加上的参数！比方说帐号、密码、读写权限等： async, sync: 此文件系统是否使用同步写入 （sync） 或非同步 （async） 的 内存机制，请参考文件系统运行方式。默认为 async。 atime,noatime: 是否修订文件的读取时间（atime）。为了性能，某些时刻可使用 noatime ro, rw: 挂载文件系统成为只读（ro） 或可读写（rw） auto, noauto: 允许此 filesystem 被以 mount -a 自动挂载（auto） dev, nodev: 是否允许此 filesystem 上，可创建设备文件？ dev 为可允许 suid, nosuid: 是否允许此 filesystem 含有 suid/sgid 的文件格式？ exec, noexec: 是否允许此 filesystem 上拥有可执行 binary 文件？ user, nouser: 是否允许此 filesystem 让任何使用者执行 mount ？一般来说， mount 仅有 root 可以进行，但下达 user 参数，则可让 一般 user 也能够对此 partition 进行 mount 。 defaults: 默认值为：rw, suid, dev, exec, auto, nouser, and async remount: 重新挂载，这在系统出错，或重新更新参数时，很有用！ umount [-fn] 设备文件名或挂载点选项与参数：-f ：强制卸载！可用在类似网络文件系统 （NFS） 无法读取到的情况下；-l ：立刻卸载文件系统，比 -f 还强！-n ：不更新 /etc/mtab 情况下卸载。 挂载vfat中文U盘1234567891011121314151617181920212223242526272829303132333435363738[root@lemon ~]# blkid/dev/sda1: UUID="1649b929-1e62-46ef-aafe-dacb17e37ce9" TYPE="xfs" /dev/mapper/centos-root: UUID="6afcb5de-f93f-4b20-b31a-4c90de872e65" TYPE="xfs" /dev/mapper/centos-swap: UUID="f6f14cdb-481c-4385-8372-96ee47214f2a" TYPE="swap" /dev/mapper/centos-home: UUID="22549e34-e3a5-4bae-96d3-702cbdcc3487" TYPE="xfs" /dev/sda2: PTTYPE="dos" /dev/sdb4: LABEL="CentOS 7 x8" UUID="B4FE-5315" TYPE="vfat" [root@lemon ~]# mount -o codepage=950,iocharset=utf8 UUID="B4FE-5315" /data/usb[root@lemon ~]# cd /data/usb[root@lemon usb]# lltotal 50036-rwxr-xr-x. 1 root root 14 Dec 10 2015 CentOS_BuildTag-rwxr-xr-x. 1 root root 1673544 Dec 1 23:17 cpu-z_1.7.8.1.exedrwxr-xr-x. 3 root root 4096 Dec 10 2015 EFI-rwxr-xr-x. 1 root root 215 Dec 10 2015 EULA-rwxr-xr-x. 1 root root 18009 Dec 10 2015 GPLdrwxr-xr-x. 3 root root 4096 Dec 10 2015 imagesdrwxr-xr-x. 2 root root 4096 Dec 10 2015 isolinuxdrwxr-xr-x. 2 root root 4096 Dec 10 2015 LiveOS-rwxr-xr-x. 1 root root 48194704 Dec 1 03:31 ludashi_baidu_5.15.16.1140.exedrwxr-xr-x. 2 root root 1298432 Dec 10 2015 Packagesdrwxr-xr-x. 2 root root 4096 Dec 10 2015 repodata-rwxr-xr-x. 1 root root 1690 Dec 10 2015 RPM-GPG-KEY-CentOS-7-rwxr-xr-x. 1 root root 1690 Dec 10 2015 RPM-GPG-KEY-CentOS-Testing-7drwxr-xr-x. 2 root root 4096 Oct 20 01:03 System Volume Information-rwxr-xr-x. 1 root root 2883 Dec 10 2015 TRANS.TBL[root@lemon usb]# df Filesystem 1K-blocks Used Available Use% Mounted on/dev/mapper/centos-root 52403200 4483080 47920120 9% /devtmpfs 908892 0 908892 0% /devtmpfs 924352 84 924268 1% /dev/shmtmpfs 924352 9104 915248 1% /runtmpfs 924352 0 924352 0% /sys/fs/cgroup/dev/mapper/centos-home 257336560 110504 257226056 1% /home/dev/sda1 508588 216628 291960 43% /boottmpfs 184872 16 184856 1% /run/user/42tmpfs 184872 0 184872 0% /run/user/1000/dev/sdb4 7897600 7729184 168416 98% /data/usb]]></content>
      <categories>
        <category>Linux 基础</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>CentOS7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph对象存储(rgw)的IPv6环境配置]]></title>
    <url>%2F2016%2F11%2F09%2FCeph%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8-rgw-IPv6%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[引言在搭建成功Ceph集群后，对于如何使用，其实我还是一脸MB的，心想竟然提供三种存储接口（对象，文件，快），口气也未免太大。在结合项目需求之后，我选择了对象存储接口。那么问题又来了，如何配置IPv6的对象存储？ 实验环境 Linux操作系统版本：CentOS Linux release 7.2.1511 (Core) Minimal镜像 603M左右 Everything镜像 7.2G左右 Ceph版本：0.94.9（hammer版本） 一个搭建成功的Ceph集群123456789[root@ceph001 ~]# ceph -s cluster 2818c750-8724-4a70-bb26-f01af7f6067f health HEALTH_OK monmap e1: 1 mons at &#123;ceph001=[2001:250:4402:2001:20c:29ff:fe25:8888]:6789/0&#125; election epoch 1, quorum 0 ceph001 osdmap e17: 3 osds: 3 up, 3 in pgmap v26: 128 pgs, 1 pools, 0 bytes data, 0 objects 101676 kB used, 284 GB / 284 GB avail 128 active+clean 具体如何搭建可以参考教程配置基于IPv6的单节点Ceph Ceph对象存储从 firefly（v0.80）版本以后，网关进程内嵌了Civetweb，而无需配置安装web服务器或者配置FastCGI，大大简化了Ceph对象网关的安装与配置。本教程亦是选用Civetweb 安装对象网关安装 ceph-radosgw12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970[root@ceph001 cluster]# yum install ceph-radosgwLoaded plugins: fastestmirror, langpacksbase | 3.6 kB 00:00:00 ceph | 2.9 kB 00:00:00 ceph-noarch | 2.9 kB 00:00:00 epel | 4.3 kB 00:00:00 extras | 3.4 kB 00:00:00 updates | 3.4 kB 00:00:00 (1/9): ceph-noarch/primary_db | 5.4 kB 00:00:00 (2/9): base/x86_64/group_gz | 155 kB 00:00:00 (3/9): epel/x86_64/group_gz | 170 kB 00:00:01 (4/9): ceph/primary_db | 160 kB 00:00:01 (5/9): extras/x86_64/primary_db | 166 kB 00:00:00 (6/9): epel/x86_64/updateinfo | 673 kB 00:00:01 (7/9): epel/x86_64/primary_db | 4.3 MB 00:00:17 (8/9): base/x86_64/primary_db | 5.3 MB 00:00:20 (9/9): updates/x86_64/primary_db | 9.1 MB 00:00:27 Determining fastest mirrorsResolving Dependencies--&gt; Running transaction check---&gt; Package ceph-radosgw.x86_64 1:0.94.9-0.el7 will be installed--&gt; Processing Dependency: mailcap for package: 1:ceph-radosgw-0.94.9-0.el7.x86_64--&gt; Processing Dependency: libfcgi.so.0()(64bit) for package: 1:ceph-radosgw-0.94.9-0.el7.x86_64--&gt; Running transaction check---&gt; Package fcgi.x86_64 0:2.4.0-25.el7 will be installed---&gt; Package mailcap.noarch 0:2.1.41-2.el7 will be installed--&gt; Finished Dependency ResolutionDependencies Resolved======================================================================================= Package Arch Version Repository Size=======================================================================================Installing: ceph-radosgw x86_64 1:0.94.9-0.el7 ceph 2.3 MInstalling for dependencies: fcgi x86_64 2.4.0-25.el7 epel 47 k mailcap noarch 2.1.41-2.el7 base 31 kTransaction Summary=======================================================================================Install 1 Package (+2 Dependent packages)Total download size: 2.4 MInstalled size: 8.6 MIs this ok [y/d/N]: yDownloading packages:(1/3): mailcap-2.1.41-2.el7.noarch.rpm | 31 kB 00:00:00 (2/3): fcgi-2.4.0-25.el7.x86_64.rpm | 47 kB 00:00:00 (3/3): ceph-radosgw-0.94.9-0.el7.x86_64.rpm | 2.3 MB 00:00:02 ---------------------------------------------------------------------------------------Total 867 kB/s | 2.4 MB 00:02 Running transaction checkRunning transaction testTransaction test succeededRunning transaction Installing : fcgi-2.4.0-25.el7.x86_64 1/3 Installing : mailcap-2.1.41-2.el7.noarch 2/3 Installing : 1:ceph-radosgw-0.94.9-0.el7.x86_64 3/3 Verifying : mailcap-2.1.41-2.el7.noarch 1/3 Verifying : 1:ceph-radosgw-0.94.9-0.el7.x86_64 2/3 Verifying : fcgi-2.4.0-25.el7.x86_64 3/3 Installed: ceph-radosgw.x86_64 1:0.94.9-0.el7 Dependency Installed: fcgi.x86_64 0:2.4.0-25.el7 mailcap.noarch 0:2.1.41-2.el7 Complete! 设置对象网关管理节点1234567891011121314151617181920ot@ceph001 cluster]# ceph-deploy admin ceph001[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (1.5.36): /usr/bin/ceph-deploy admin ceph001[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] overwrite_conf : False[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe09a5280e0&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] client : ['ceph001'][ceph_deploy.cli][INFO ] func : &lt;function admin at 0x7fe09b38f410&gt;[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph001[ceph001][DEBUG ] connection detected need for sudo[ceph001][DEBUG ] connected to host: ceph001 [ceph001][DEBUG ] detect platform information from remote host[ceph001][DEBUG ] detect machine type[ceph001][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf 新建网关实例123456789101112131415161718192021222324252627282930313233[root@ceph001 cluster]# ceph-deploy rgw create ceph001[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (1.5.36): /usr/bin/ceph-deploy rgw create ceph001[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] rgw : [('ceph001', 'rgw.ceph001')][ceph_deploy.cli][INFO ] overwrite_conf : False[ceph_deploy.cli][INFO ] subcommand : create[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x2a7ee60&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] func : &lt;function rgw at 0x29e7230&gt;[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.rgw][DEBUG ] Deploying rgw, cluster ceph hosts ceph001:rgw.ceph001[ceph001][DEBUG ] connection detected need for sudo[ceph001][DEBUG ] connected to host: ceph001 [ceph001][DEBUG ] detect platform information from remote host[ceph001][DEBUG ] detect machine type[ceph_deploy.rgw][INFO ] Distro info: CentOS Linux 7.2.1511 Core[ceph_deploy.rgw][DEBUG ] remote host will use sysvinit[ceph_deploy.rgw][DEBUG ] deploying rgw bootstrap to ceph001[ceph001][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[ceph001][DEBUG ] create path recursively if it doesn't exist[ceph001][INFO ] Running command: sudo ceph --cluster ceph --name client.bootstrap-rgw --keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring auth get-or-create client.rgw.ceph001 osd allow rwx mon allow rw -o /var/lib/ceph/radosgw/ceph-rgw.ceph001/keyring[ceph001][INFO ] Running command: sudo service ceph-radosgw start[ceph001][DEBUG ] Reloading systemd: [ OK ][ceph001][DEBUG ] Starting ceph-radosgw (via systemctl): [ OK ][ceph001][INFO ] Running command: sudo systemctl enable ceph-radosgw[ceph001][WARNIN] ceph-radosgw.service is not a native service, redirecting to /sbin/chkconfig.[ceph001][WARNIN] Executing /sbin/chkconfig ceph-radosgw on[ceph_deploy.rgw][INFO ] The Ceph Object Gateway (RGW) is now running on host ceph001 and default port 7480 ceph-radosgw守护进程 Civetweb Webserver 默认运行在端口7480上，可以通过以下命令查看12[root@ceph001 ceph]# netstat -nlp | grep 7480tcp 0 0 0.0.0.0:7480 0.0.0.0:* LISTEN 3537/radosgw 修改默认端口，并配置IPv6网关12345678910111213141516171819202122232425262728293031323334[root@ceph001 cluster]# vim ceph.conf[global]fsid = 2818c750-8724-4a70-bb26-f01af7f6067fms_bind_ipv6 = truemon_initial_members = ceph001mon_host = [2001:250:4402:2001:20c:29ff:fe25:8888]auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxosd_pool_default_size = 1# IPv6网关配置[client.rgw.ceph001]rgw_frontends= "civetweb port=[::]:80"[root@ceph001 cluster]# ceph-deploy --overwrite-conf config push ceph001[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (1.5.36): /usr/bin/ceph-deploy --overwrite-conf config push ceph001[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] overwrite_conf : True[ceph_deploy.cli][INFO ] subcommand : push[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fdc0728a7a0&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] client : ['ceph001'][ceph_deploy.cli][INFO ] func : &lt;function config at 0x7fdc072652a8&gt;[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.config][DEBUG ] Pushing config to ceph001[ceph001][DEBUG ] connection detected need for sudo[ceph001][DEBUG ] connected to host: ceph001 [ceph001][DEBUG ] detect platform information from remote host[ceph001][DEBUG ] detect machine type[ceph001][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf 重启服务，查看ceph-radosgw守护进程12[root@ceph001 ceph]# netstat -nlp | grep 80tcp6 0 0 :::80 :::* LISTEN 3540/ra 可以通过浏览器来访问的IPv6对象网关(http://[2001:250:4402:2001:20c:29ff:fe25:8888]/)得到与下图类似返回结果恭喜你，完成了IPv6的网关配置，更多内容可以参考官网Ceph对象网关 简单使用新建一个用户(S3接口）123456789101112131415161718192021222324252627282930313233[root@ceph001 ~]# radosgw-admin user create --uid=lemon --display-name="柠檬" --email=lemon@qq.com&#123; "user_id": "lemon", "display_name": "柠檬", "email": "lemon@qq.com", "suspended": 0, "max_buckets": 1000, "auid": 0, "subusers": [], "keys": [ &#123; "user": "lemon", "access_key": "29YAB6D3BVRBQQDFVLHI", "secret_key": "QVPTxEvZHxQJhNdR58tZCfsgyP37jOKBKiPg1TaU" &#125; ], "swift_keys": [], "caps": [], "op_mask": "read, write, delete", "default_placement": "", "placement_tags": [], "bucket_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "user_quota": &#123; "enabled": false, "max_size_kb": -1, "max_objects": -1 &#125;, "temp_url_keys": []&#125; 生成的access_key与secret_key可供访问任何兼容 S3 API 的客户端能够使用。更多配置请参考官网 管理手册 通过CloudBerry Explorer for Amazon S3 客户端验证IPv6平台的部署情况下载该测试客户端CloudBerry Explorer for Amazon S3 for Windows 官网 百度云 密码: bxu5 安装成功后启动客户端，得到类似如下界面 添加创建成功的Ceph对象存储 点击菜单栏 File-&gt;Edit Accounts点击 Add-&gt;S3 Compatible配置相关参数，如下图所示，然后点击测试连接(test connection) Display name：随便填 Service point：即我们的对象网关地址，比如这里是http://[2001:250:4402:2001:20c:29ff:fe25:8888]/ Access key: 为我们创建用户的access_key，比如这里是29YAB6D3BVRBQQDFVLHI Secret key: 为我们创建用户的secret_key, 比如这里是QVPTxEvZHxQJhNdR58tZCfsgyP37jOKBKiPg1TaU 如果测试成功会弹出一个 connection success对话框 使用选择刚刚创建的账户ceph001接下来可以通过该客户端创建bucket，上传以及下载文件等操作,其他就不一一介绍了ceph端可以通过ceph -w 查看实时的客户端操作，比如这里是客户端进行写操作123456789101112131415161718192021222324252627282930313233[root@ceph001 ~]# ceph -w cluster 2818c750-8724-4a70-bb26-f01af7f6067f health HEALTH_OK monmap e1: 1 mons at &#123;ceph001=[2001:250:4402:2001:20c:29ff:fe25:8888]:6789/0&#125; election epoch 1, quorum 0 ceph001 osdmap e54: 3 osds: 3 up, 3 in pgmap v312: 200 pgs, 10 pools, 425 MB data, 164 objects 826 MB used, 284 GB / 284 GB avail 200 active+clean client io 11630 kB/s wr, 25 op/s2016-11-10 10:30:23.606053 mon.0 [INF] pgmap v312: 200 pgs: 200 active+clean; 425 MB data, 826 MB used, 284 GB / 284 GB avail; 11630 kB/s wr, 25 op/s2016-11-10 10:30:27.197373 mon.0 [INF] pgmap v313: 200 pgs: 200 active+clean; 443 MB data, 866 MB used, 284 GB / 284 GB avail; 7826 kB/s wr, 17 op/s2016-11-10 10:30:28.810358 mon.0 [INF] pgmap v314: 200 pgs: 200 active+clean; 457 MB data, 910 MB used, 283 GB / 284 GB avail; 5914 kB/s wr, 12 op/s2016-11-10 10:30:31.830126 mon.0 [INF] pgmap v315: 200 pgs: 200 active+clean; 466 MB data, 927 MB used, 283 GB / 284 GB avail; 5015 kB/s wr, 11 op/s2016-11-10 10:30:32.918332 mon.0 [INF] pgmap v316: 200 pgs: 200 active+clean; 502 MB data, 1011 MB used, 283 GB / 284 GB avail; 10214 kB/s wr, 22 op/s2016-11-10 10:30:37.113515 mon.0 [INF] pgmap v317: 200 pgs: 200 active+clean; 521 MB data, 1037 MB used, 283 GB / 284 GB avail; 11093 kB/s wr, 24 op/s2016-11-10 10:30:38.256587 mon.0 [INF] pgmap v318: 200 pgs: 200 active+clean; 563 MB data, 1082 MB used, 283 GB / 284 GB avail; 11669 kB/s wr, 25 op/s2016-11-10 10:30:42.089761 mon.0 [INF] pgmap v319: 200 pgs: 200 active+clean; 580 MB data, 1106 MB used, 283 GB / 284 GB avail; 11572 kB/s wr, 25 op/s2016-11-10 10:30:43.099061 mon.0 [INF] pgmap v320: 200 pgs: 200 active+clean; 609 MB data, 1162 MB used, 283 GB / 284 GB avail; 9575 kB/s wr, 21 op/s2016-11-10 10:30:47.423680 mon.0 [INF] pgmap v321: 200 pgs: 200 active+clean; 628 MB data, 1184 MB used, 283 GB / 284 GB avail; 9104 kB/s wr, 19 op/s2016-11-10 10:30:48.938458 mon.0 [INF] pgmap v322: 200 pgs: 200 active+clean; 652 MB data, 1212 MB used, 283 GB / 284 GB avail; 7914 kB/s wr, 17 op/s2016-11-10 10:30:49.948222 mon.0 [INF] pgmap v323: 200 pgs: 200 active+clean; 660 MB data, 1216 MB used, 283 GB / 284 GB avail; 13007 kB/s wr, 28 op/s2016-11-10 10:30:52.843301 mon.0 [INF] pgmap v324: 200 pgs: 200 active+clean; 668 MB data, 1238 MB used, 283 GB / 284 GB avail; 3975 kB/s wr, 8 op/s2016-11-10 10:30:54.968022 mon.0 [INF] pgmap v325: 200 pgs: 200 active+clean; 714 MB data, 1278 MB used, 283 GB / 284 GB avail; 12919 kB/s wr, 28 op/s2016-11-10 10:30:58.521788 mon.0 [INF] pgmap v326: 200 pgs: 200 active+clean; 730 MB data, 1290 MB used, 283 GB / 284 GB avail; 12325 kB/s wr, 27 op/s2016-11-10 10:30:59.558175 mon.0 [INF] pgmap v327: 200 pgs: 200 active+clean; 764 MB data, 1334 MB used, 283 GB / 284 GB avail; 9621 kB/s wr, 20 op/s2016-11-10 10:31:03.218629 mon.0 [INF] pgmap v328: 200 pgs: 200 active+clean; 776 MB data, 1354 MB used, 283 GB / 284 GB avail; 8880 kB/s wr, 19 op/s2016-11-10 10:31:04.234516 mon.0 [INF] pgmap v329: 200 pgs: 200 active+clean; 816 MB data, 1382 MB used, 283 GB / 284 GB avail; 11345 kB/s wr, 24 op/s2016-11-10 10:31:08.422236 mon.0 [INF] pgmap v330: 200 pgs: 200 active+clean; 820 MB data, 1398 MB used, 283 GB / 284 GB avail; 8706 kB/s wr, 19 op/s2016-11-10 10:31:09.870466 mon.0 [INF] pgmap v331: 200 pgs: 200 active+clean; 856 MB data, 1463 MB used, 283 GB / 284 GB avail; 7887 kB/s wr, 17 op/s2016-11-10 10:31:14.003109 mon.0 [INF] pgmap v332: 200 pgs: 200 active+clean; 884 MB data, 1491 MB used, 283 GB / 284 GB avail; 12958 kB/s wr, 28 op/s... 如果是想开发属于自己的S3客户端，可以调用相关的S3 API，具体可以参考官网S3 API希望能帮到大家]]></content>
      <categories>
        <category>Ceph</category>
      </categories>
      <tags>
        <tag>IPv6</tag>
        <tag>Ceph</tag>
        <tag>rgw</tag>
        <tag>对象存储</tag>
        <tag>云存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置基于IPv6的单节点Ceph]]></title>
    <url>%2F2016%2F11%2F06%2F%E9%85%8D%E7%BD%AE%E5%9F%BA%E4%BA%8EIPv6%E7%9A%84Ceph%2F</url>
    <content type="text"><![CDATA[引言为什么突然想起搭建一个基于IPv6的Ceph环境？纯属巧合，原本有一个项目需要搭建一个基于IPv6的文件系统，可惜Hadoop不支持（之前一直觉得Hadoop比较强大），几经折腾，Ceph给了我希望，好了闲话少说，直接进入正题。 实验环境 Linux操作系统版本：CentOS Linux release 7.2.1511 (Core) Minimal镜像 603M左右 Everything镜像 7.2G左右 Ceph版本：0.94.9（hammer版本） 原本选取的为jewel最新版本，环境配置成功后，在使用Ceph的对象存储功能时，导致不能通过IPv6访问，出现类似如下错误提示,查阅资料发现是Ceph jewel版本的一个bug，正在修复，另外也给大家一个建议，在生产环境中，尽量不要选择最新版本。 set_ports_option:[::]8888:invalid port sport spec 预检网络配置参考之前的一篇文章CentOS7 设置静态IPv6/IPv4地址完成网络配置 修改主机名12345[root@localhost ~]# hostnamectl set-hostname ceph001 #ceph001即为你想要修改的名字[root@localhost ~]# vim /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain62001:250:4402:2001:20c:29ff:fe25:8888 ceph001 #新增，前面IPv6地址即主机ceph001的静态IPv6地址 修改yum源由于某些原因，可能导致官方的yum在下载软件时速度较慢，这里我们将yum源换为aliyun源1234567[root@localhost ~]# yum clean all #清空yum源[root@localhost ~]# rm -rf /etc/yum.repos.d/*.repo[root@localhost ~]# wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo #下载阿里base源[root@localhost ~]# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo #下载阿里epel源[root@localhost ~]# sed -i '/aliyuncs/d' /etc/yum.repos.d/CentOS-Base.repo[root@localhost ~]# sed -i '/aliyuncs/d' /etc/yum.repos.d/epel.repo[root@localhost ~]# sed -i 's/$releasever/7.2.1511/g' /etc/yum.repos.d/CentOS-Base.repo 添加ceph源12345678910[root@localhost ~]# vim /etc/yum.repos.d/ceph.repo[ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el7/x86_64/ #可以选择需要安装的版本gpgcheck=0[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el7/noarch/ #可以选择需要安装的版本gpgcheck=0[root@localhost ~]# yum makecache 安装ceph与ceph-deploy12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@localhost ~]# yum install ceph ceph-deployLoaded plugins: fastestmirror, langpacksLoading mirror speeds from cached hostfileResolving Dependencies--&gt; Running transaction check---&gt; Package ceph.x86_64 1:0.94.9-0.el7 will be installed--&gt; Processing Dependency: librbd1 = 1:0.94.9-0.el7 for package: 1:ceph-0.94.9-0.el7.x86_64--&gt; Processing Dependency: python-rbd = 1:0.94.9-0.el7 for package: 1:ceph-0.94.9-0.el7.x86_64--&gt; Processing Dependency: python-cephfs = 1:0.94.9-0.el7 for package: 1:ceph-0.94.9-0.el7.x86_64--&gt; Processing Dependency: libcephfs1 = 1:0.94.9-0.el7 for package: 1:ceph-0.94.9-0.el7.x86_64--&gt; Processing Dependency: librados2 = 1:0.94.9-0.el7 for package: 1:ceph-0.94.9-0.el7.x86_64--&gt; Processing Dependency: python-rados = 1:0.94.9-0.el7 for package: 1:ceph-0.94.9-0.el7.x86_64--&gt; Processing Dependency: ceph-common = 1:0.94.9-0.el7 for package: 1:ceph-0.94.9-0.el7.x86_64--&gt; Processing Dependency: python-requests for package: 1:ceph-0.94.9-0.el7.x86_64--&gt; Processing Dependency: python-flask for package: 1:ceph-0.94.9-0.el7.x86_64--&gt; Processing Dependency: redhat-lsb-core for package: 1:ceph-0.94.9-0.el7.x86_64--&gt; Processing Dependency: hdparm for package: 1:ceph-0.94.9-0.el7.x86_64--&gt; Processing Dependency: libcephfs.so.1()(64bit) for package: 1:ceph-0.94.9-0.el7.x86_64.......Dependencies Resolved======================================================================================= Package Arch Version Repository Size=======================================================================================Installing: ceph x86_64 1:0.94.9-0.el7 ceph 20 M ceph-deploy noarch 1.5.36-0 ceph-noarch 283 kInstalling for dependencies: boost-program-options x86_64 1.53.0-25.el7 base 155 k ceph-common x86_64 1:0.94.9-0.el7 ceph 7.2 M...Transaction Summary=======================================================================================Install 2 Packages (+24 Dependent packages)Upgrade ( 2 Dependent packages)Total download size: 37 MIs this ok [y/d/N]: yDownloading packages:No Presto metadata available for cephwarning: /var/cache/yum/x86_64/7/base/packages/boost-program-options-1.53.0-25.el7.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEYPublic key for boost-program-options-1.53.0-25.el7.x86_64.rpm is not installed(1/28): boost-program-options-1.53.0-25.el7.x86_64.rpm | 155 kB 00:00:00 (2/28): hdparm-9.43-5.el7.x86_64.rpm | 83 kB 00:00:00 (3/28): ceph-deploy-1.5.36-0.noarch.rpm | 283 kB 00:00:00 (4/28): leveldb-1.12.0-11.el7.x86_64.rpm | 161 kB 00:00:00 ... ---------------------------------------------------------------------------------------Total 718 kB/s | 37 MB 00:53 Retrieving key from http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7Importing GPG key 0xF4A80EB5: Userid : "CentOS-7 Key (CentOS 7 Official Signing Key) &lt;security@centos.org&gt;" Fingerprint: 6341 ab27 53d7 8a78 a7c2 7bb1 24c6 a8a7 f4a8 0eb5 From : http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7Is this ok [y/N]: y...Complete! 验证安装版本1234[root@localhost ~]# ceph-deploy --version1.5.36[root@localhost ~]# ceph -vceph version 0.94.9 (fe6d859066244b97b24f09d46552afc2071e6f90) 安装NTP（如果是多节点还需要配置服务端与客户端），并设置selinux与firewalld1234567[root@localhost ~]# yum install ntp[root@localhost ~]# sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/selinux/config[root@localhost ~]# setenforce 0[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# systemctl disable firewalldRemoved symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.Removed symlink /etc/systemd/system/basic.target.wants/firewalld.service. 创建Ceph集群在管理节点（ceph001）[root@ceph001 ~]# mkdir cluster[root@ceph001 ~]# cd cluster/ 创建集群12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@ceph001 cluster]# ceph-deploy new ceph001[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (1.5.36): /usr/bin/ceph-deploy new ceph001[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] func : &lt;function new at 0xfe0668&gt;[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] overwrite_conf : False[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x104c680&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] ssh_copykey : True[ceph_deploy.cli][INFO ] mon : ['ceph001'][ceph_deploy.cli][INFO ] public_network : None[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] cluster_network : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.cli][INFO ] fsid : None[ceph_deploy.new][DEBUG ] Creating new cluster named ceph[ceph_deploy.new][INFO ] making sure passwordless SSH succeeds[ceph001][DEBUG ] connected to host: ceph001 [ceph001][DEBUG ] detect platform information from remote host[ceph001][DEBUG ] detect machine type[ceph001][DEBUG ] find the location of an executable[ceph001][INFO ] Running command: /usr/sbin/ip link show[ceph001][INFO ] Running command: /usr/sbin/ip addr show[ceph001][DEBUG ] IP addresses found: [u'192.168.122.1', u'49.123.105.124'][ceph_deploy.new][DEBUG ] Resolving host ceph001[ceph_deploy.new][DEBUG ] Monitor ceph001 at 2001:250:4402:2001:20c:29ff:fe25:8888[ceph_deploy.new][INFO ] Monitors are IPv6, binding Messenger traffic on IPv6[ceph_deploy.new][DEBUG ] Monitor initial members are ['ceph001'][ceph_deploy.new][DEBUG ] Monitor addrs are ['[2001:250:4402:2001:20c:29ff:fe25:8888]'][ceph_deploy.new][DEBUG ] Creating a random mon key...[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...[root@ceph001 cluster]# lltotal 12-rw-r--r--. 1 root root 244 Nov 6 21:54 ceph.conf-rw-r--r--. 1 root root 3106 Nov 6 21:54 ceph-deploy-ceph.log-rw-------. 1 root root 73 Nov 6 21:54 ceph.mon.keyring[root@ceph001 cluster]# cat ceph.conf [global]fsid = 865e6b01-b0ea-44da-87a5-26a4980aa7a8ms_bind_ipv6 = truemon_initial_members = ceph001mon_host = [2001:250:4402:2001:20c:29ff:fe25:8888]auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephx 由于我们采用的单节点部署，将默认的复制备份数改为1（原本是3）123456789101112131415161718192021[root@ceph001 cluster]# echo "osd_pool_default_size = 1" &gt;&gt; ceph.conf[root@ceph001 cluster]# ceph-deploy --overwrite-conf config push ceph001[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (1.5.36): /usr/bin/ceph-deploy --overwrite-conf config push ceph001[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] overwrite_conf : True[ceph_deploy.cli][INFO ] subcommand : push[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x14f9710&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] client : ['ceph001'][ceph_deploy.cli][INFO ] func : &lt;function config at 0x14d42a8&gt;[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.config][DEBUG ] Pushing config to ceph001[ceph001][DEBUG ] connected to host: ceph001 [ceph001][DEBUG ] detect platform information from remote host[ceph001][DEBUG ] detect machine type[ceph001][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf 创建监控节点将ceph001作为监控节点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106[root@ceph001 cluster]# ceph-deploy mon create-initial[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (1.5.36): /usr/bin/ceph-deploy mon create-initial[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] overwrite_conf : False[ceph_deploy.cli][INFO ] subcommand : create-initial[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x23865a8&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] func : &lt;function mon at 0x237e578&gt;[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.cli][INFO ] keyrings : None[ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts ceph001[ceph_deploy.mon][DEBUG ] detecting platform for host ceph001 ...[ceph001][DEBUG ] connected to host: ceph001 [ceph001][DEBUG ] detect platform information from remote host[ceph001][DEBUG ] detect machine type[ceph001][DEBUG ] find the location of an executable[ceph_deploy.mon][INFO ] distro info: CentOS Linux 7.2.1511 Core[ceph001][DEBUG ] determining if provided host has same hostname in remote[ceph001][DEBUG ] get remote short hostname[ceph001][DEBUG ] deploying mon to ceph001[ceph001][DEBUG ] get remote short hostname[ceph001][DEBUG ] remote hostname: ceph001[ceph001][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[ceph001][DEBUG ] create the mon path if it does not exist[ceph001][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-ceph001/done[ceph001][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-ceph001/done[ceph001][INFO ] creating keyring file: /var/lib/ceph/tmp/ceph-ceph001.mon.keyring[ceph001][DEBUG ] create the monitor keyring file[ceph001][INFO ] Running command: ceph-mon --cluster ceph --mkfs -i ceph001 --keyring /var/lib/ceph/tmp/ceph-ceph001.mon.keyring[ceph001][DEBUG ] ceph-mon: mon.noname-a [2001:250:4402:2001:20c:29ff:fe25:8888]:6789/0 is local, renaming to mon.ceph001[ceph001][DEBUG ] ceph-mon: set fsid to 865e6b01-b0ea-44da-87a5-26a4980aa7a8[ceph001][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-ceph001 for mon.ceph001[ceph001][INFO ] unlinking keyring file /var/lib/ceph/tmp/ceph-ceph001.mon.keyring[ceph001][DEBUG ] create a done file to avoid re-doing the mon deployment[ceph001][DEBUG ] create the init path if it does not exist[ceph001][DEBUG ] locating the `service` executable...[ceph001][INFO ] Running command: /usr/sbin/service ceph -c /etc/ceph/ceph.conf start mon.ceph001[ceph001][DEBUG ] === mon.ceph001 === [ceph001][DEBUG ] Starting Ceph mon.ceph001 on ceph001...[ceph001][WARNIN] Running as unit ceph-mon.ceph001.1478441156.735105300.service.[ceph001][DEBUG ] Starting ceph-create-keys on ceph001...[ceph001][INFO ] Running command: systemctl enable ceph[ceph001][WARNIN] ceph.service is not a native service, redirecting to /sbin/chkconfig.[ceph001][WARNIN] Executing /sbin/chkconfig ceph on[ceph001][INFO ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph001.asok mon_status[ceph001][DEBUG ] ********************************************************************************[ceph001][DEBUG ] status for monitor: mon.ceph001[ceph001][DEBUG ] &#123;[ceph001][DEBUG ] "election_epoch": 2, [ceph001][DEBUG ] "extra_probe_peers": [], [ceph001][DEBUG ] "monmap": &#123;[ceph001][DEBUG ] "created": "0.000000", [ceph001][DEBUG ] "epoch": 1, [ceph001][DEBUG ] "fsid": "865e6b01-b0ea-44da-87a5-26a4980aa7a8", [ceph001][DEBUG ] "modified": "0.000000", [ceph001][DEBUG ] "mons": [[ceph001][DEBUG ] &#123;[ceph001][DEBUG ] "addr": "[2001:250:4402:2001:20c:29ff:fe25:8888]:6789/0", [ceph001][DEBUG ] "name": "ceph001", [ceph001][DEBUG ] "rank": 0[ceph001][DEBUG ] &#125;[ceph001][DEBUG ] ][ceph001][DEBUG ] &#125;, [ceph001][DEBUG ] "name": "ceph001", [ceph001][DEBUG ] "outside_quorum": [], [ceph001][DEBUG ] "quorum": [[ceph001][DEBUG ] 0[ceph001][DEBUG ] ], [ceph001][DEBUG ] "rank": 0, [ceph001][DEBUG ] "state": "leader", [ceph001][DEBUG ] "sync_provider": [][ceph001][DEBUG ] &#125;[ceph001][DEBUG ] ********************************************************************************[ceph001][INFO ] monitor: mon.ceph001 is running[ceph001][INFO ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph001.asok mon_status[ceph_deploy.mon][INFO ] processing monitor mon.ceph001[ceph001][DEBUG ] connected to host: ceph001 [ceph001][DEBUG ] detect platform information from remote host[ceph001][DEBUG ] detect machine type[ceph001][DEBUG ] find the location of an executable[ceph001][INFO ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.ceph001.asok mon_status[ceph_deploy.mon][INFO ] mon.ceph001 monitor has reached quorum![ceph_deploy.mon][INFO ] all initial monitors are running and have formed quorum[ceph_deploy.mon][INFO ] Running gatherkeys...[ceph_deploy.gatherkeys][INFO ] Storing keys in temp directory /tmp/tmpgY2IT7[ceph001][DEBUG ] connected to host: ceph001 [ceph001][DEBUG ] detect platform information from remote host[ceph001][DEBUG ] detect machine type[ceph001][DEBUG ] get remote short hostname[ceph001][DEBUG ] fetch remote file[ceph001][INFO ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.ceph001.asok mon_status[ceph001][INFO ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-ceph001/keyring auth get client.admin[ceph001][INFO ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-ceph001/keyring auth get client.bootstrap-mds[ceph001][INFO ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-ceph001/keyring auth get client.bootstrap-osd[ceph001][INFO ] Running command: /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-ceph001/keyring auth get client.bootstrap-rgw[ceph_deploy.gatherkeys][INFO ] Storing ceph.client.admin.keyring[ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-mds.keyring[ceph_deploy.gatherkeys][INFO ] keyring 'ceph.mon.keyring' already exists[ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-osd.keyring[ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-rgw.keyring[ceph_deploy.gatherkeys][INFO ] Destroy temp directory /tmp/tmpgY2IT7 查看集群状态123456789101112[root@ceph001 cluster]# ceph -s cluster 865e6b01-b0ea-44da-87a5-26a4980aa7a8 health HEALTH_ERR 64 pgs stuck inactive 64 pgs stuck unclean no osds monmap e1: 1 mons at &#123;ceph001=[2001:250:4402:2001:20c:29ff:fe25:8888]:6789/0&#125; election epoch 2, quorum 0 ceph001 osdmap e1: 0 osds: 0 up, 0 in pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects 0 kB used, 0 kB / 0 kB avail 64 creating 添加OSD查看硬盘123456789101112131415161718192021222324252627282930[root@ceph001 cluster]# ceph-deploy disk list ceph001[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (1.5.36): /usr/bin/ceph-deploy disk list ceph001[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] overwrite_conf : False[ceph_deploy.cli][INFO ] subcommand : list[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x1c79bd8&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] func : &lt;function disk at 0x1c70e60&gt;[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.cli][INFO ] disk : [('ceph001', None, None)][ceph001][DEBUG ] connected to host: ceph001 [ceph001][DEBUG ] detect platform information from remote host[ceph001][DEBUG ] detect machine type[ceph001][DEBUG ] find the location of an executable[ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.2.1511 Core[ceph_deploy.osd][DEBUG ] Listing disks on ceph001...[ceph001][DEBUG ] find the location of an executable[ceph001][INFO ] Running command: /usr/sbin/ceph-disk list[ceph001][DEBUG ] /dev/sda :[ceph001][DEBUG ] /dev/sda1 other, xfs, mounted on /boot[ceph001][DEBUG ] /dev/sda2 other, LVM2_member[ceph001][DEBUG ] /dev/sdb other, unknown[ceph001][DEBUG ] /dev/sdc other, unknown[ceph001][DEBUG ] /dev/sdd other, unknown[ceph001][DEBUG ] /dev/sr0 other, iso9660 添加第一个OSD（/dev/sdb）123456789101112131415161718192021222324252627282930[root@ceph001 cluster]# ceph-deploy disk zap ceph001:/dev/sdb[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (1.5.36): /usr/bin/ceph-deploy disk zap ceph001:/dev/sdb[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] overwrite_conf : False[ceph_deploy.cli][INFO ] subcommand : zap[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x1b14bd8&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] func : &lt;function disk at 0x1b0be60&gt;[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.cli][INFO ] disk : [('ceph001', '/dev/sdb', None)][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on ceph001[ceph001][DEBUG ] connected to host: ceph001 [ceph001][DEBUG ] detect platform information from remote host[ceph001][DEBUG ] detect machine type[ceph001][DEBUG ] find the location of an executable[ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.2.1511 Core[ceph001][DEBUG ] zeroing last few blocks of device[ceph001][DEBUG ] find the location of an executable[ceph001][INFO ] Running command: /usr/sbin/ceph-disk zap /dev/sdb[ceph001][DEBUG ] Creating new GPT entries.[ceph001][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or[ceph001][DEBUG ] other utilities.[ceph001][DEBUG ] Creating new GPT entries.[ceph001][DEBUG ] The operation has completed successfully.[ceph001][WARNIN] partx: specified range &lt;1:0&gt; does not make sense 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293[root@ceph001 cluster]# ceph-deploy osd create ceph001:/dev/sdb[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (1.5.36): /usr/bin/ceph-deploy osd create ceph001:/dev/sdb[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] disk : [('ceph001', '/dev/sdb', None)][ceph_deploy.cli][INFO ] dmcrypt : False[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] bluestore : None[ceph_deploy.cli][INFO ] overwrite_conf : False[ceph_deploy.cli][INFO ] subcommand : create[ceph_deploy.cli][INFO ] dmcrypt_key_dir : /etc/ceph/dmcrypt-keys[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x19b6680&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] fs_type : xfs[ceph_deploy.cli][INFO ] func : &lt;function osd at 0x19aade8&gt;[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.cli][INFO ] zap_disk : False[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks ceph001:/dev/sdb:[ceph001][DEBUG ] connected to host: ceph001 [ceph001][DEBUG ] detect platform information from remote host[ceph001][DEBUG ] detect machine type[ceph001][DEBUG ] find the location of an executable[ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.2.1511 Core[ceph_deploy.osd][DEBUG ] Deploying osd to ceph001[ceph001][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[ceph_deploy.osd][DEBUG ] Preparing host ceph001 disk /dev/sdb journal None activate True[ceph001][DEBUG ] find the location of an executable[ceph001][INFO ] Running command: /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /dev/sdb[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_cryptsetup_parameters[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_key_size[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_dmcrypt_type[ceph001][WARNIN] INFO:ceph-disk:Will colocate journal with data on /dev/sdb[ceph001][WARNIN] DEBUG:ceph-disk:Creating journal partition num 2 size 5120 on /dev/sdb[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/sbin/sgdisk --new=2:0:5120M --change-name=2:ceph journal --partition-guid=2:ae307314-3a81-4da2-974b-b21c24d9bba1 --typecode=2:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdb[ceph001][DEBUG ] The operation has completed successfully.[ceph001][WARNIN] INFO:ceph-disk:calling partx on prepared device /dev/sdb[ceph001][WARNIN] INFO:ceph-disk:re-reading known partitions will display errors[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/sbin/partx -a /dev/sdb[ceph001][WARNIN] partx: /dev/sdb: error adding partition 2[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/bin/udevadm settle[ceph001][WARNIN] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/ae307314-3a81-4da2-974b-b21c24d9bba1[ceph001][WARNIN] DEBUG:ceph-disk:Journal is GPT partition /dev/disk/by-partuuid/ae307314-3a81-4da2-974b-b21c24d9bba1[ceph001][WARNIN] DEBUG:ceph-disk:Creating osd partition on /dev/sdb[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/sbin/sgdisk --largest-new=1 --change-name=1:ceph data --partition-guid=1:16a6298d-59bb-4190-867a-10a5b519e7c0 --typecode=1:89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be -- /dev/sdb[ceph001][DEBUG ] The operation has completed successfully.[ceph001][WARNIN] INFO:ceph-disk:calling partx on created device /dev/sdb[ceph001][WARNIN] INFO:ceph-disk:re-reading known partitions will display errors[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/sbin/partx -a /dev/sdb[ceph001][WARNIN] partx: /dev/sdb: error adding partitions 1-2[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/bin/udevadm settle[ceph001][WARNIN] DEBUG:ceph-disk:Creating xfs fs on /dev/sdb1[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/sbin/mkfs -t xfs -f -i size=2048 -- /dev/sdb1[ceph001][DEBUG ] meta-data=/dev/sdb1 isize=2048 agcount=4, agsize=6225855 blks[ceph001][DEBUG ] = sectsz=512 attr=2, projid32bit=1[ceph001][DEBUG ] = crc=0 finobt=0[ceph001][DEBUG ] data = bsize=4096 blocks=24903419, imaxpct=25[ceph001][DEBUG ] = sunit=0 swidth=0 blks[ceph001][DEBUG ] naming =version 2 bsize=4096 ascii-ci=0 ftype=0[ceph001][DEBUG ] log =internal log bsize=4096 blocks=12159, version=2[ceph001][DEBUG ] = sectsz=512 sunit=0 blks, lazy-count=1[ceph001][DEBUG ] realtime =none extsz=4096 blocks=0, rtextents=0[ceph001][WARNIN] DEBUG:ceph-disk:Mounting /dev/sdb1 on /var/lib/ceph/tmp/mnt.2SMGIk with options noatime,inode64[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/bin/mount -t xfs -o noatime,inode64 -- /dev/sdb1 /var/lib/ceph/tmp/mnt.2SMGIk[ceph001][WARNIN] DEBUG:ceph-disk:Preparing osd data dir /var/lib/ceph/tmp/mnt.2SMGIk[ceph001][WARNIN] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/tmp/mnt.2SMGIk/journal -&gt; /dev/disk/by-partuuid/ae307314-3a81-4da2-974b-b21c24d9bba1[ceph001][WARNIN] DEBUG:ceph-disk:Unmounting /var/lib/ceph/tmp/mnt.2SMGIk[ceph001][WARNIN] INFO:ceph-disk:Running command: /bin/umount -- /var/lib/ceph/tmp/mnt.2SMGIk[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb[ceph001][DEBUG ] Warning: The kernel is still using the old partition table.[ceph001][DEBUG ] The new table will be used at the next reboot.[ceph001][DEBUG ] The operation has completed successfully.[ceph001][WARNIN] INFO:ceph-disk:calling partx on prepared device /dev/sdb[ceph001][WARNIN] INFO:ceph-disk:re-reading known partitions will display errors[ceph001][WARNIN] INFO:ceph-disk:Running command: /usr/sbin/partx -a /dev/sdb[ceph001][WARNIN] partx: /dev/sdb: error adding partitions 1-2[ceph001][INFO ] Running command: systemctl enable ceph[ceph001][WARNIN] ceph.service is not a native service, redirecting to /sbin/chkconfig.[ceph001][WARNIN] Executing /sbin/chkconfig ceph on[ceph001][INFO ] checking OSD status...[ceph001][DEBUG ] find the location of an executable[ceph001][INFO ] Running command: /bin/ceph --cluster=ceph osd stat --format=json[ceph001][WARNIN] there is 1 OSD down[ceph001][WARNIN] there is 1 OSD out[ceph_deploy.osd][DEBUG ] Host ceph001 is now ready for osd use. 查看集群状态1234567891011[root@ceph001 cluster]# ceph -s cluster 865e6b01-b0ea-44da-87a5-26a4980aa7a8 health HEALTH_WARN 64 pgs stuck inactive 64 pgs stuck unclean monmap e1: 1 mons at &#123;ceph001=[2001:250:4402:2001:20c:29ff:fe25:8888]:6789/0&#125; election epoch 1, quorum 0 ceph001 osdmap e3: 1 osds: 0 up, 0 in pgmap v4: 64 pgs, 1 pools, 0 bytes data, 0 objects 0 kB used, 0 kB / 0 kB avail 64 creating 继续添加其他OSD123456789101112131415[root@ceph001 cluster]# ceph-deploy disk zap ceph001:/dev/sdc[root@ceph001 cluster]# ceph-deploy disk zap ceph001:/dev/sdd[root@ceph001 cluster]# ceph-deploy osd create ceph001:/dev/sdc[root@ceph001 cluster]# ceph-deploy osd create ceph001:/dev/sdd[root@ceph001 cluster]# ceph -s cluster 865e6b01-b0ea-44da-87a5-26a4980aa7a8 health HEALTH_WARN 64 pgs stuck inactive 64 pgs stuck unclean monmap e1: 1 mons at &#123;ceph001=[2001:250:4402:2001:20c:29ff:fe25:8888]:6789/0&#125; election epoch 1, quorum 0 ceph001 osdmap e7: 3 osds: 0 up, 0 in pgmap v8: 64 pgs, 1 pools, 0 bytes data, 0 objects 0 kB used, 0 kB / 0 kB avail 64 creating 重启机器，查看集群状态12345678910[root@ceph001 ~]# ceph -s cluster 2818c750-8724-4a70-bb26-f01af7f6067f health HEALTH_WARN too few PGs per OSD (21 &lt; min 30) monmap e1: 1 mons at &#123;ceph001=[2001:250:4402:2001:20c:29ff:fe25:8888]:6789/0&#125; election epoch 1, quorum 0 ceph001 osdmap e9: 3 osds: 3 up, 3 in pgmap v11: 64 pgs, 1 pools, 0 bytes data, 0 objects 102196 kB used, 284 GB / 284 GB avail 64 active+clean 错误处理我们可以看到，目前集群状态为HEALTH_WARN，存在以下警告提示1too few PGs per OSD (21 &lt; min 30) 增大rbd的pg数(too few PGs per OSD (21 &lt; min 30))1234[root@ceph001 cluster]# ceph osd pool set rbd pg_num 128set pool 0 pg_num to 128[root@ceph001 cluster]# ceph osd pool set rbd pgp_num 128set pool 0 pgp_num to 128 查看集群状态123456789[root@ceph001 ~]# ceph -s cluster 2818c750-8724-4a70-bb26-f01af7f6067f health HEALTH_OK monmap e1: 1 mons at &#123;ceph001=[2001:250:4402:2001:20c:29ff:fe25:8888]:6789/0&#125; election epoch 1, quorum 0 ceph001 osdmap e13: 3 osds: 3 up, 3 in pgmap v17: 128 pgs, 1 pools, 0 bytes data, 0 objects 101544 kB used, 284 GB / 284 GB avail 128 active+clean 小结 本教程只是简单的搭建了一个单节点的Ceph环境，如果要换成多节点也很简单，操作大同小异 在基于IPv6的Ceph配置上，个人觉得与IPv4操作相差不大，只需要注意两点 配置静态的IPv6地址 修改主机名并添加域名解析，将主机名对应于前面设置的静态IPv6地址]]></content>
      <categories>
        <category>Ceph</category>
      </categories>
      <tags>
        <tag>IPv6</tag>
        <tag>Ceph</tag>
        <tag>云存储</tag>
        <tag>单节点部署ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7 设置静态IPv6/IPv4地址]]></title>
    <url>%2F2016%2F11%2F06%2FStatic-IPv6%2F</url>
    <content type="text"><![CDATA[1 环境准备实验均在Vmware Workstation虚拟机上完成，上网方式选择桥接模式，保证网络支持IPv6,为了操作方便，我们使用root用户登录系统 1.1 Linux系统版本CentOS Linux release 7.2.1511 (Core)12[root@localhost ~]# cat /etc/redhat-releaseCentOS Linux release 7.2.1511 (Core) 1.2 系统支持IPv612345678910[root@localhost ~]# ifconfigeno16777736: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 49.123.105.124 netmask 255.255.192.0 broadcast 49.123.127.255 inet6 fe80::20c:29ff:fe25:c621 prefixlen 64 scopeid 0x20&lt;link&gt; inet6 2001:250:4402:2001:20c:29ff:fe25:c621 prefixlen 64 scopeid 0x0&lt;global&gt; ether 00:0c:29:25:c6:21 txqueuelen 1000 (Ethernet) RX packets 19255 bytes 2006024 (1.9 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 2151 bytes 210339 (205.4 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 如图所示，可以看到系统默认的IPv6地址为2001:250:4402:2001:20c:29ff:fe25:c621 2 修改网络配置文件根据1.2，我们得知网卡为eno16777736，切换到目录/etc/sysconfig/network-scripts下1[root@localhost ~]# cd /etc/sysconfig/network-scripts 编辑文件 ifcfg-eno16777736123456789101112131415161718[root@localhost ~]# vim ifcfg-eno16777736TYPE=EthernetBOOTPROTO=noneDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=noIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noNAME=eno16777736UUID=13faf306-5205-4d3a-ac23-4699236dee95DEVICE=eno16777736ONBOOT=yesDNS1=202.197.96.1IPV6ADDR=2001:250:4402:2001:20c:29ff:fe25:8888/64IPADDR=49.123.105.124PREFIX=8GATEWAY=49.123.64.1 3 重启网络1[root@localhost network-scripts]# systemctl restart network 4 测试是否配置成功4.1 通过ifconfig命令查看设置的IPv6/IPv4地址1234567891011[root@localhost ~]# ifconfig eno16777736eno16777736: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 49.123.105.124 netmask 255.0.0.0 broadcast 49.255.255.255 inet6 2001:250:4402:2001:20c:29ff:fe25:8888 prefixlen 64 scopeid 0x0&lt;global&gt; inet6 fe80::20c:29ff:fe25:c621 prefixlen 64 scopeid 0x20&lt;link&gt; inet6 2001:250:4402:2001:20c:29ff:fe25:c621 prefixlen 64 scopeid 0x0&lt;global&gt; ether 00:0c:29:25:c6:21 txqueuelen 1000 (Ethernet) RX packets 33732 bytes 8234767 (7.8 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 6229 bytes 623829 (609.2 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 4.2 使用另外一台机器进行ping连接测试（两台机器均在同一局域网内）IPv4测试1234567891011[root@localhost ~]# ping 49.123.105.124PING 49.123.105.124 (49.123.105.124) 56(84) bytes of data.64 bytes from 49.123.105.124: icmp_seq=1 ttl=64 time=0.075 ms64 bytes from 49.123.105.124: icmp_seq=2 ttl=64 time=0.074 ms64 bytes from 49.123.105.124: icmp_seq=3 ttl=64 time=0.092 ms64 bytes from 49.123.105.124: icmp_seq=4 ttl=64 time=0.066 ms64 bytes from 49.123.105.124: icmp_seq=5 ttl=64 time=0.067 ms^C--- 49.123.105.124 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 4004msrtt min/avg/max/mdev = 0.066/0.074/0.092/0.014 ms IPv6测试12345678910[root@localhost ~]# ping6 2001:250:4402:2001:20c:29ff:fe25:8888PING 2001:250:4402:2001:20c:29ff:fe25:8888(2001:250:4402:2001:20c:29ff:fe25:8888) 56 data bytes64 bytes from 2001:250:4402:2001:20c:29ff:fe25:8888: icmp_seq=1 ttl=64 time=0.099 ms64 bytes from 2001:250:4402:2001:20c:29ff:fe25:8888: icmp_seq=2 ttl=64 time=0.110 ms64 bytes from 2001:250:4402:2001:20c:29ff:fe25:8888: icmp_seq=3 ttl=64 time=0.126 ms64 bytes from 2001:250:4402:2001:20c:29ff:fe25:8888: icmp_seq=4 ttl=64 time=0.110 ms64 bytes from 2001:250:4402:2001:20c:29ff:fe25:8888: icmp_seq=5 ttl=64 time=0.113 ms^C--- 2001:250:4402:2001:20c:29ff:fe25:8888 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 4003ms 恭喜你，到此已经完成了CentOS 7的静态IPv6/IPV4配置]]></content>
      <categories>
        <category>Linux 基础</category>
      </categories>
      <tags>
        <tag>IPv6</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
</search>